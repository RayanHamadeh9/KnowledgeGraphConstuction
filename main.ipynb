{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Verifying that all libraries are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are installed!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "required_packages =[\n",
    "    'beautifulsoup4', 'spacy', 'nltk', 'sklearn-crfsuite', 'datasets', 'transformers', 'rdflib', 'requests'\n",
    "]\n",
    "\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = {pkg for pkg in required_packages if pkg.lower() not in installed}\n",
    "\n",
    "if missing:\n",
    "    print(f\"Missing packages: {missing}\")\n",
    "    # pip install {' '.join(missing)}\n",
    "\n",
    "else: \n",
    "    print(\"All required packages are installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading necessary NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading spacy models...\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m730.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading spacy models...\")\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Basic text cleaning\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove numbers but keep hyphenated words\n",
    "    text = re.sub(r'(?<!\\w)[-+]?\\d+', '', text)\n",
    "\n",
    "    # Keep hyphens for compound words but remove other punctuation\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    More advanced preprocessing with tokenization, stopword removal, and lemmatization \n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text first\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join back to text\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Apple Inc. was founded by Steve Jobs in 1976. Their headquarters is in Cupertino, California. The CEO is Tim Cook.\n",
      "Cleaned: apple inc was founded by steve jobs in their headquarters is in cupertino california the ceo is tim cook\n",
      "Preprocessed: apple inc founded steve job headquarters cupertino california ceo tim cook\n"
     ]
    }
   ],
   "source": [
    "# Testing the functions\n",
    "sample_text = \"Apple Inc. was founded by Steve Jobs in 1976. Their headquarters is in Cupertino, California. The CEO is Tim Cook.\"\n",
    "print (\"Original:\", sample_text)\n",
    "print (\"Cleaned:\", clean_text(sample_text))\n",
    "print (\"Preprocessed:\", preprocess_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAmed Entry Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CoNLL-2003 dataset...\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading CoNLL-2003 dataset...\")\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extraction functions for CRF\n",
    "def sent2features(sentence_data):\n",
    "    \"\"\"Extract features for each word in the sentence.\"\"\"\n",
    "    tokens = sentence_data['tokens']\n",
    "    pos_tags = sentence_data['pos_tags']\n",
    "    \n",
    "    features = []\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        postag = str(pos_tags[i])  # Convert to string to be safe\n",
    "        \n",
    "        word_features = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word[-3:]': word[-3:] if len(word) > 3 else word,\n",
    "            'word[-2:]': word[-2:] if len(word) > 2 else word,\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'postag': postag,\n",
    "        }\n",
    "        \n",
    "        # Features for words that are not at the beginning of a document\n",
    "        if i > 0:\n",
    "            word1 = tokens[i-1]\n",
    "            postag1 = str(pos_tags[i-1])\n",
    "            word_features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:postag': postag1,\n",
    "            })\n",
    "        else:\n",
    "            # Indicate that this is the beginning of a document\n",
    "            word_features['BOS'] = True\n",
    "        \n",
    "        # Features for words that are not at the end of a document\n",
    "        if i < len(tokens) - 1:\n",
    "            word1 = tokens[i+1]\n",
    "            postag1 = str(pos_tags[i+1])\n",
    "            word_features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:postag': postag1,\n",
    "            })\n",
    "        else:\n",
    "            # Indicate that this is the end of a document\n",
    "            word_features['EOS'] = True\n",
    "        \n",
    "        features.append(word_features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sentence_data):\n",
    "    \"\"\"Extract NER labels for each word in the sentence.\"\"\"\n",
    "    return [str(label) for label in sentence_data['ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for CRF model...\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for CRF\n",
    "print(\"Preparing data for CRF model...\")\n",
    "# Using smaller subsets for faster training and evaluation\n",
    "X_train = [sent2features(sentence) for sentence in train_dataset.select(range(1000))]\n",
    "y_train = [sent2labels(sentence) for sentence in train_dataset.select(range(1000))]\n",
    "\n",
    "X_test = [sent2features(sentence) for sentence in test_dataset.select(range(100))]\n",
    "y_test = [sent2labels(sentence) for sentence in test_dataset.select(range(100))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CRF model (this might take a while)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=False, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=False, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=False, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train CRF model\n",
    "print(\"Training CRF model (this might take a while)...\")\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CRF model...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1086\n",
      "           1       0.94      0.85      0.89       111\n",
      "           2       0.92      0.98      0.95        97\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.84      0.79      0.82        82\n",
      "           6       0.50      0.22      0.31         9\n",
      "           7       0.95      0.86      0.90        22\n",
      "           8       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.96      1422\n",
      "   macro avg       0.67      0.62      0.64      1422\n",
      "weighted avg       0.96      0.96      0.96      1422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CRF model\n",
    "print(\"Evaluating CRF model...\")\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map CoNLL-2003 numeric labels to text labels\n",
    "def map_conll_labels(label_id):\n",
    "    \"\"\"Map CoNLL-2003 numeric labels to text labels.\"\"\"\n",
    "    label_map = {\n",
    "        '0': 'O',       # Outside of a named entity\n",
    "        '1': 'B-PER',   # Beginning of person name\n",
    "        '2': 'I-PER',   # Inside of person name\n",
    "        '3': 'B-ORG',   # Beginning of organization name\n",
    "        '4': 'I-ORG',   # Inside of organization name\n",
    "        '5': 'B-LOC',   # Beginning of location name\n",
    "        '6': 'I-LOC',   # Inside of location name\n",
    "        '7': 'B-MISC',  # Beginning of miscellaneous entity\n",
    "        '8': 'I-MISC'   # Inside of miscellaneous entity\n",
    "    }\n",
    "    return label_map.get(label_id, label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy NER model...\n",
      "The spaCy en_ner_conll03 model isn't installed. Using en_core_web_sm instead.\n",
      "Successfully loaded en_core_web_sm model\n"
     ]
    }
   ],
   "source": [
    "# Using spaCy's pre-trained NER model\n",
    "print(\"Loading spaCy NER model...\")\n",
    "try:\n",
    "    nlp_ner = spacy.load(\"en_ner_conll03\")\n",
    "    print(\"Successfully loaded en_ner_conll03 model\")\n",
    "except OSError:\n",
    "    print(\"The spaCy en_ner_conll03 model isn't installed. Using en_core_web_sm instead.\")\n",
    "    try:\n",
    "        nlp_ner = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"Successfully loaded en_core_web_sm model\")\n",
    "    except OSError:\n",
    "        print(\"Downloading en_core_web_sm model...\")\n",
    "        spacy.cli.download(\"en_core_web_sm\")\n",
    "        nlp_ner = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_spacy(text):\n",
    "    \"\"\"Extract named entities using spaCy.\"\"\"\n",
    "    doc = nlp_ner(text)\n",
    "    entities = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities_crf(text, crf_model):\n",
    "    \"\"\"\n",
    "    Predict named entities in text using trained CRF model.\n",
    "    \"\"\"\n",
    "    # Tokenize text (simple split for basic demonstration)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Use spaCy for better tokenization and POS tagging\n",
    "    doc = nlp_ner(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Create features that match our training features more closely\n",
    "    features = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_features = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower(),\n",
    "            'word[-3:]': token[-3:] if len(token) > 3 else token,\n",
    "            'word[-2:]': token[-2:] if len(token) > 2 else token,\n",
    "            'word.isupper()': token.isupper(),\n",
    "            'word.istitle()': token.istitle(),\n",
    "            'word.isdigit()': token.isdigit(),\n",
    "            'postag': pos_tags[i],\n",
    "        }\n",
    "        \n",
    "        # Add features for previous word if not at beginning\n",
    "        if i > 0:\n",
    "            token_features.update({\n",
    "                '-1:word.lower()': tokens[i-1].lower(),\n",
    "                '-1:word.istitle()': tokens[i-1].istitle(),\n",
    "                '-1:word.isupper()': tokens[i-1].isupper(),\n",
    "                '-1:postag': pos_tags[i-1],\n",
    "            })\n",
    "        else:\n",
    "            token_features['BOS'] = True\n",
    "            \n",
    "        # Add features for next word if not at end\n",
    "        if i < len(tokens) - 1:\n",
    "            token_features.update({\n",
    "                '+1:word.lower()': tokens[i+1].lower(),\n",
    "                '+1:word.istitle()': tokens[i+1].istitle(),\n",
    "                '+1:word.isupper()': tokens[i+1].isupper(),\n",
    "                '+1:postag': pos_tags[i+1],\n",
    "            })\n",
    "        else:\n",
    "            token_features['EOS'] = True\n",
    "            \n",
    "        features.append(token_features)\n",
    "    \n",
    "    # Make prediction\n",
    "    if features:  # Check if we have any features to predict on\n",
    "        predictions = crf_model.predict([features])[0]\n",
    "        \n",
    "        # For debugging\n",
    "        print(\"DEBUG - Tokens:\", tokens)\n",
    "        print(\"DEBUG - Predictions:\", predictions)\n",
    "        \n",
    "        # Combine tokens with their predicted labels\n",
    "        entities = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if predictions[i].startswith('B-') or (predictions[i] != '0' and predictions[i] != 'O'):\n",
    "                # Handle both string labels and numeric indices from CoNLL\n",
    "                entity_type = predictions[i][2:] if predictions[i].startswith('B-') else map_conll_labels(predictions[i])\n",
    "                entity_text = tokens[i]\n",
    "                j = i + 1\n",
    "                \n",
    "                # Continue collecting tokens that are part of the same entity\n",
    "                while j < len(tokens) and (predictions[j].startswith('I-') or predictions[j] == str(int(predictions[i]) + 1)):\n",
    "                    entity_text += ' ' + tokens[j]\n",
    "                    j += 1\n",
    "                \n",
    "                # Add the entity to our list\n",
    "                start_pos = text.find(tokens[i])\n",
    "                end_pos = start_pos + len(entity_text)\n",
    "                entities.append((entity_text, entity_type, start_pos, end_pos))\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return entities\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text: Apple was founded by Steve Jobs. The company is headquartered in Cupertino, California.\n",
      "\n",
      "spaCy NER results:\n",
      "Entity: Apple, Type: ORG, Position: 0-5\n",
      "Entity: Steve Jobs, Type: PERSON, Position: 21-31\n",
      "Entity: Cupertino, Type: GPE, Position: 65-74\n",
      "Entity: California, Type: GPE, Position: 76-86\n",
      "\n",
      "CRF model results:\n",
      "DEBUG - Tokens: ['Apple', 'was', 'founded', 'by', 'Steve', 'Jobs', '.', 'The', 'company', 'is', 'headquartered', 'in', 'Cupertino', ',', 'California', '.']\n",
      "DEBUG - Predictions: ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '5' '0']\n",
      "Entity: California, Type: B-LOC, Position: 76-86\n"
     ]
    }
   ],
   "source": [
    "# Compare models on a sample text\n",
    "sample_text = \"Apple was founded by Steve Jobs. The company is headquartered in Cupertino, California.\"\n",
    "print(\"\\nSample text:\", sample_text)\n",
    "\n",
    "print(\"\\nspaCy NER results:\")\n",
    "spacy_entities = extract_entities_spacy(sample_text)\n",
    "for entity in spacy_entities:\n",
    "    print(f\"Entity: {entity[0]}, Type: {entity[1]}, Position: {entity[2]}-{entity[3]}\")\n",
    "\n",
    "print(\"\\nCRF model results:\")\n",
    "crf_entities = predict_entities_crf(sample_text, crf)\n",
    "for entity in crf_entities:\n",
    "    print(f\"Entity: {entity[0]}, Type: {entity[1]}, Position: {entity[2]}-{entity[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing more samples:\n",
      "\n",
      "Sample 1: Microsoft CEO Satya Nadella announced new products at the conference in Seattle.\n",
      "spaCy entities:\n",
      "  Microsoft (ORG)\n",
      "  Satya Nadella (PERSON)\n",
      "  Seattle (GPE)\n",
      "CRF entities:\n",
      "DEBUG - Tokens: ['Microsoft', 'CEO', 'Satya', 'Nadella', 'announced', 'new', 'products', 'at', 'the', 'conference', 'in', 'Seattle', '.']\n",
      "DEBUG - Predictions: ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      "\n",
      "Sample 2: Tesla and SpaceX are companies founded by Elon Musk who lives in Texas.\n",
      "spaCy entities:\n",
      "  Elon Musk (PERSON)\n",
      "  Texas (GPE)\n",
      "CRF entities:\n",
      "DEBUG - Tokens: ['Tesla', 'and', 'SpaceX', 'are', 'companies', 'founded', 'by', 'Elon', 'Musk', 'who', 'lives', 'in', 'Texas', '.']\n",
      "DEBUG - Predictions: ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      "\n",
      "Sample 3: Amazon has its headquarters in Seattle, Washington, which is a city in the United States.\n",
      "spaCy entities:\n",
      "  Amazon (ORG)\n",
      "  Seattle (GPE)\n",
      "  Washington (GPE)\n",
      "  the United States (GPE)\n",
      "CRF entities:\n",
      "DEBUG - Tokens: ['Amazon', 'has', 'its', 'headquarters', 'in', 'Seattle', ',', 'Washington', ',', 'which', 'is', 'a', 'city', 'in', 'the', 'United', 'States', '.']\n",
      "DEBUG - Predictions: ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "# Evaluate further sample texts\n",
    "print(\"\\nTesting more samples:\")\n",
    "test_samples = [\n",
    "    \"Microsoft CEO Satya Nadella announced new products at the conference in Seattle.\",\n",
    "    \"Tesla and SpaceX are companies founded by Elon Musk who lives in Texas.\",\n",
    "    \"Amazon has its headquarters in Seattle, Washington, which is a city in the United States.\"\n",
    "]\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"\\nSample {i+1}: {sample}\")\n",
    "    print(\"spaCy entities:\")\n",
    "    for entity in extract_entities_spacy(sample):\n",
    "        print(f\"  {entity[0]} ({entity[1]})\")\n",
    "    \n",
    "    print(\"CRF entities:\")\n",
    "    for entity in predict_entities_crf(sample, crf):\n",
    "        print(f\"  {entity[0]} ({entity[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully implemented both NER approaches but the spaCy model outperformed the CRF model out-of-the-box, this is likely due to its extensive pretraining.\n",
    "\n",
    "The CRF model would likely require more data and tuning to acheive comparable results.\n",
    "\n",
    "For practicality we decided to continue with spaCy for the remainder of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation Extraction with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations(text):\n",
    "    \"\"\"Extract relations between entities using a more comprehensive approach.\"\"\"\n",
    "    doc = nlp_ner(text)\n",
    "    relations = []\n",
    "    \n",
    "    # Get all named entities\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    entity_tokens = {}\n",
    "    \n",
    "    # Map entity texts to their tokens\n",
    "    for ent in doc.ents:\n",
    "        for token in ent:\n",
    "            entity_tokens[token.i] = (ent.text, ent.label_)\n",
    "    \n",
    "    # Find subject-verb-object patterns\n",
    "    for sent in doc.sents:\n",
    "        # First, get all potential subjects (entities that are subjects in the sentence)\n",
    "        subjects = []\n",
    "        for token in sent:\n",
    "            # Check if token is a subject and part of a named entity\n",
    "            if token.dep_ in (\"nsubj\", \"nsubjpass\") and token.i in entity_tokens:\n",
    "                subjects.append((token, entity_tokens[token.i]))\n",
    "        \n",
    "        # For each potential subject\n",
    "        for subj_token, subj_entity in subjects:\n",
    "            verb = subj_token.head  # The verb is the head of the subject\n",
    "            \n",
    "            # Look for direct objects\n",
    "            for token in verb.children:\n",
    "                # Direct object\n",
    "                if token.dep_ in (\"dobj\", \"attr\", \"pobj\") and token.i in entity_tokens:\n",
    "                    obj_entity = entity_tokens[token.i]\n",
    "                    relations.append({\n",
    "                        \"subject\": subj_entity[0],\n",
    "                        \"subject_type\": subj_entity[1],\n",
    "                        \"predicate\": verb.text,\n",
    "                        \"object\": obj_entity[0],\n",
    "                        \"object_type\": obj_entity[1]\n",
    "                    })\n",
    "                \n",
    "                # Prepositional phrase\n",
    "                elif token.dep_ == \"prep\":\n",
    "                    prep = token\n",
    "                    for pobj in prep.children:\n",
    "                        if pobj.dep_ == \"pobj\" and pobj.i in entity_tokens:\n",
    "                            obj_entity = entity_tokens[pobj.i]\n",
    "                            relations.append({\n",
    "                                \"subject\": subj_entity[0],\n",
    "                                \"subject_type\": subj_entity[1],\n",
    "                                \"predicate\": f\"{verb.text} {prep.text}\",\n",
    "                                \"object\": obj_entity[0],\n",
    "                                \"object_type\": obj_entity[1]\n",
    "                            })\n",
    "    \n",
    "    for ent1 in doc.ents:\n",
    "        for ent2 in doc.ents:\n",
    "            if ent1.text != ent2.text:  # Don't relate an entity to itself\n",
    "                # Find a path connecting these entities\n",
    "                for token in ent1:\n",
    "                    # Look at verbs connected to this entity\n",
    "                    if token.head.pos_ == \"VERB\":\n",
    "                        verb = token.head\n",
    "                        # Look for prepositions in the verb's children\n",
    "                        for child in verb.children:\n",
    "                            if child.dep_ == \"prep\":\n",
    "                                prep = child\n",
    "                                # Check if this prep connects to the second entity\n",
    "                                for token2 in ent2:\n",
    "                                    if token2.head == prep:\n",
    "                                        relations.append({\n",
    "                                            \"subject\": ent1.text,\n",
    "                                            \"subject_type\": ent1.label_,\n",
    "                                            \"predicate\": f\"{verb.text} {prep.text}\",\n",
    "                                            \"object\": ent2.text,\n",
    "                                            \"object_type\": ent2.label_\n",
    "                                        })\n",
    "    \n",
    "    prev_ent = None\n",
    "    for ent in doc.ents:\n",
    "        if prev_ent:\n",
    "            # Check if they're close to each other\n",
    "            if ent.start - prev_ent.end <= 5:  # Within 5 tokens\n",
    "                # Check for specific patterns like apposition\n",
    "                for token in doc[prev_ent.start:ent.start]:\n",
    "                    if token.dep_ == \"appos\" or token.text in [\"of\", \"at\", \"in\", \"from\", \"by\"]:\n",
    "                        relations.append({\n",
    "                            \"subject\": prev_ent.text,\n",
    "                            \"subject_type\": prev_ent.label_,\n",
    "                            \"predicate\": \"related_to\",  # Generic relation\n",
    "                            \"object\": ent.text,\n",
    "                            \"object_type\": ent.label_\n",
    "                        })\n",
    "        prev_ent = ent\n",
    "    \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing relation extraction:\n",
      "\n",
      "Sample 1: Microsoft CEO Satya Nadella announced new products at the conference in Seattle.\n",
      "Entities:\n",
      "  Microsoft (ORG)\n",
      "  Satya Nadella (PERSON)\n",
      "  Seattle (GPE)\n",
      "Relations:\n",
      "  No relations found\n",
      "\n",
      "Sample 2: Tesla and SpaceX are companies founded by Elon Musk who lives in Texas.\n",
      "Entities:\n",
      "  Elon Musk (PERSON)\n",
      "  Texas (GPE)\n",
      "Relations:\n",
      "  Elon Musk (PERSON) --related_to--> Texas (GPE)\n",
      "\n",
      "Sample 3: Amazon has its headquarters in Seattle, Washington, which is a city in the United States.\n",
      "Entities:\n",
      "  Amazon (ORG)\n",
      "  Seattle (GPE)\n",
      "  Washington (GPE)\n",
      "  the United States (GPE)\n",
      "Relations:\n",
      "  Amazon (ORG) --related_to--> Seattle (GPE)\n"
     ]
    }
   ],
   "source": [
    "# Test the relation extraction on our sample texts\n",
    "print(\"\\nTesting relation extraction:\")\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"\\nSample {i+1}: {sample}\")\n",
    "    entities = extract_entities_spacy(sample)\n",
    "    print(\"Entities:\")\n",
    "    for entity in entities:\n",
    "        print(f\"  {entity[0]} ({entity[1]})\")\n",
    "    \n",
    "    relations = extract_relations(sample)\n",
    "    print(\"Relations:\")\n",
    "    if relations:\n",
    "        for relation in relations:\n",
    "            print(f\"  {relation['subject']} ({relation['subject_type']}) --{relation['predicate']}--> {relation['object']} ({relation['object_type']})\")\n",
    "    else:\n",
    "        print(\"  No relations found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_uri(text):\n",
    "    \"\"\"Convert text to a valid URI component.\"\"\"\n",
    "    # Remove special characters and spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Replace spaces with underscores and convert to lowercase\n",
    "    return text.replace(' ', '_').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph(text):\n",
    "    \"\"\"\n",
    "    Build a knowledge graph from text using spaCy for entity and relation extraction.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to process\n",
    "        \n",
    "    Returns:\n",
    "        An RDFLib Graph object\n",
    "    \"\"\"\n",
    "    # Create a new RDF graph\n",
    "    g = Graph()\n",
    "    \n",
    "    # Define namespaces\n",
    "    EX = Namespace(\"http://example.org/\")\n",
    "    g.bind(\"ex\", EX)\n",
    "    \n",
    "    # Extract entities and relations\n",
    "    entities = extract_entities_spacy(text)\n",
    "    relations = extract_relations(text)\n",
    "    \n",
    "    # Process entities\n",
    "    entity_uris = {}\n",
    "    for entity_text, entity_type, start, end in entities:\n",
    "        # Create a URI for the entity\n",
    "        entity_uri = URIRef(EX[text_to_uri(entity_text)])\n",
    "        entity_uris[entity_text] = entity_uri\n",
    "        \n",
    "        # Add entity type information\n",
    "        if entity_type == \"PERSON\":\n",
    "            g.add((entity_uri, RDF.type, EX.Person))\n",
    "        elif entity_type == \"ORG\":\n",
    "            g.add((entity_uri, RDF.type, EX.Organization))\n",
    "        elif entity_type in [\"GPE\", \"LOC\"]:\n",
    "            g.add((entity_uri, RDF.type, EX.Location))\n",
    "        else:\n",
    "            g.add((entity_uri, RDF.type, EX.Entity))\n",
    "        \n",
    "        # Add entity label\n",
    "        g.add((entity_uri, RDFS.label, Literal(entity_text)))\n",
    "    \n",
    "    # Process relations\n",
    "    for relation in relations:\n",
    "        subject_text = relation[\"subject\"]\n",
    "        object_text = relation[\"object\"]\n",
    "        predicate = relation[\"predicate\"]\n",
    "        \n",
    "        # Get or create URIs for subject and object\n",
    "        if subject_text in entity_uris:\n",
    "            subject_uri = entity_uris[subject_text]\n",
    "        else:\n",
    "            # Create a URI if not already created\n",
    "            subject_uri = URIRef(EX[text_to_uri(subject_text)])\n",
    "            g.add((subject_uri, RDFS.label, Literal(subject_text)))\n",
    "        \n",
    "        if object_text in entity_uris:\n",
    "            object_uri = entity_uris[object_text]\n",
    "        else:\n",
    "            # Create a URI if not already created\n",
    "            object_uri = URIRef(EX[text_to_uri(object_text)])\n",
    "            g.add((object_uri, RDFS.label, Literal(object_text)))\n",
    "        \n",
    "        # Create a predicate URI\n",
    "        predicate_uri = URIRef(EX[text_to_uri(predicate)])\n",
    "        \n",
    "        # Add the relation triple\n",
    "        g.add((subject_uri, predicate_uri, object_uri))\n",
    "    \n",
    "    return g, entities, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Star Wars example text...\n",
      "\n",
      "Extracted entities from Star Wars text:\n",
      "Entity: Movie, Type: NORP\n",
      "Entity: Jedis, Type: PERSON\n",
      "Entity: Luke, Type: PERSON\n",
      "Entity: Jedi, Type: PERSON\n",
      "Entity: Master Yoda, Type: PERSON\n",
      "Entity: Jedi, Type: PERSON\n",
      "Entity: Chewbacca, Type: GPE\n",
      "Entity: Han, Type: NORP\n",
      "Entity: Falcon, Type: ORG\n",
      "Entity: Millennium Falcon, Type: FAC\n",
      "Entity: 1.5, Type: CARDINAL\n",
      "\n",
      "Extracted relations from Star Wars text:\n",
      "Luke (PERSON) --is--> Jedi (PERSON)\n",
      "Master Yoda (PERSON) --is--> Jedi (PERSON)\n",
      "Falcon (ORG) --related_to--> Millennium Falcon (FAC)\n",
      "\n",
      "Knowledge graph triples:\n",
      "Jedis --22-rdf-syntax-ns#type--> http://example.org/Person\n",
      "Falcon --rdf-schema#label--> Falcon\n",
      "Master Yoda --22-rdf-syntax-ns#type--> http://example.org/Person\n",
      "Master Yoda --rdf-schema#label--> Master Yoda\n",
      "Movie --22-rdf-syntax-ns#type--> http://example.org/Entity\n",
      "Millennium Falcon --22-rdf-syntax-ns#type--> http://example.org/Entity\n",
      "1.5 --rdf-schema#label--> 1.5\n",
      "Han --rdf-schema#label--> Han\n",
      "Movie --rdf-schema#label--> Movie\n",
      "Chewbacca --rdf-schema#label--> Chewbacca\n",
      "Han --22-rdf-syntax-ns#type--> http://example.org/Entity\n",
      "Falcon --22-rdf-syntax-ns#type--> http://example.org/Organization\n",
      "Millennium Falcon --rdf-schema#label--> Millennium Falcon\n",
      "Jedi --rdf-schema#label--> Jedi\n",
      "Chewbacca --22-rdf-syntax-ns#type--> http://example.org/Location\n",
      "Jedis --rdf-schema#label--> Jedis\n",
      "Luke --is--> Jedi\n",
      "Jedi --22-rdf-syntax-ns#type--> http://example.org/Person\n",
      "1.5 --22-rdf-syntax-ns#type--> http://example.org/Entity\n",
      "Falcon --related_to--> Millennium Falcon\n",
      "Luke --22-rdf-syntax-ns#type--> http://example.org/Person\n",
      "Master Yoda --is--> Jedi\n",
      "Luke --rdf-schema#label--> Luke\n"
     ]
    }
   ],
   "source": [
    "star_wars_text = \"\"\"Star Wars IV is a Movie where there are different kinds of creatures, like\n",
    "humans and wookies. Some creatures are Jedis; for instance, the human Luke\n",
    "is a Jedi, and Master Yoda – for whom the species is not known – is also a\n",
    "Jedi. The wookie named Chewbacca is Han's co-pilot on the Millennium\n",
    "Falcon starship. The speed of Millennium Falcon is 1.5 (above the speed of\n",
    "light!)\"\"\"\n",
    "\n",
    "print(\"\\nProcessing Star Wars example text...\")\n",
    "kg, entities, relations = build_knowledge_graph(star_wars_text)\n",
    "\n",
    "print(\"\\nExtracted entities from Star Wars text:\")\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity[0]}, Type: {entity[1]}\")\n",
    "\n",
    "print(\"\\nExtracted relations from Star Wars text:\")\n",
    "if relations:\n",
    "    for relation in relations:\n",
    "        print(f\"{relation['subject']} ({relation['subject_type']}) --{relation['predicate']}--> {relation['object']} ({relation['object_type']})\")\n",
    "else:\n",
    "    print(\"No relations found\")\n",
    "\n",
    "print(\"\\nKnowledge graph triples:\")\n",
    "for s, p, o in kg:\n",
    "    s_label = kg.value(s, RDFS.label) or s\n",
    "    p_label = p.split('/')[-1]\n",
    "    o_label = kg.value(o, RDFS.label) or o\n",
    "    print(f\"{s_label} --{p_label}--> {o_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing SPARQL queries on the knowledge graph:\n",
      "\n",
      "Query 1: All Person entities\n",
      "Person: Jedis\n",
      "Person: Luke\n",
      "Person: Jedi\n",
      "Person: Master Yoda\n",
      "\n",
      "Query 2: Relationships between entities\n",
      "Luke --http://example.org/is--> Jedi\n",
      "Master Yoda --http://example.org/is--> Jedi\n",
      "Falcon --http://example.org/related_to--> Millennium Falcon\n",
      "\n",
      "Query 3: Entity type counts\n",
      "Entity: <built-in method count of ResultRow object at 0x319e76cc0>\n",
      "Person: <built-in method count of ResultRow object at 0x319e76db0>\n",
      "Location: <built-in method count of ResultRow object at 0x319e76cc0>\n",
      "Organization: <built-in method count of ResultRow object at 0x319e76db0>\n"
     ]
    }
   ],
   "source": [
    "# Perform SPARQL queries\n",
    "print(\"\\nPerforming SPARQL queries on the knowledge graph:\")\n",
    "\n",
    "# Query 1: Find all entities of a specific type\n",
    "query1 = \"\"\"\n",
    "SELECT ?entity ?label\n",
    "WHERE {\n",
    "    ?entity a <http://example.org/Person> .\n",
    "    ?entity <http://www.w3.org/2000/01/rdf-schema#label> ?label .\n",
    "}\n",
    "\"\"\"\n",
    "print(\"\\nQuery 1: All Person entities\")\n",
    "results1 = list(kg.query(query1))\n",
    "if results1:\n",
    "    for row in results1:\n",
    "        print(f\"Person: {row.label}\")\n",
    "else:\n",
    "    print(\"No Person entities found\")\n",
    "\n",
    "# Query 2: Find relationships between entities\n",
    "query2 = \"\"\"\n",
    "SELECT ?s_label ?p ?o_label\n",
    "WHERE {\n",
    "    ?s ?p ?o .\n",
    "    ?s <http://www.w3.org/2000/01/rdf-schema#label> ?s_label .\n",
    "    ?o <http://www.w3.org/2000/01/rdf-schema#label> ?o_label .\n",
    "    FILTER(?p != <http://www.w3.org/2000/01/rdf-schema#label>)\n",
    "    FILTER(?p != <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)\n",
    "}\n",
    "\"\"\"\n",
    "print(\"\\nQuery 2: Relationships between entities\")\n",
    "results2 = list(kg.query(query2))\n",
    "if results2:\n",
    "    for row in results2:\n",
    "        print(f\"{row.s_label} --{row.p}--> {row.o_label}\")\n",
    "else:\n",
    "    print(\"No relationships found\")\n",
    "\n",
    "# Query 3: Find all entity types and their counts\n",
    "query3 = \"\"\"\n",
    "SELECT ?type (COUNT(?entity) as ?count)\n",
    "WHERE {\n",
    "    ?entity a ?type .\n",
    "}\n",
    "GROUP BY ?type\n",
    "\"\"\"\n",
    "print(\"\\nQuery 3: Entity type counts\")\n",
    "for row in kg.query(query3):\n",
    "    type_name = row.type.split('/')[-1]\n",
    "    print(f\"{type_name}: {row.count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is performing well enough!\n",
    "\n",
    "- **Query 1:** successfully found Person entities: Jedis, Luke, Jedi, and Master Yoda\n",
    "- **Query 2:** found meaningfull relations\n",
    "  - Luke is Jedi\n",
    "  - Master Yoda is Jedi\n",
    "  - Falcon is related to Millenium Falcon\n",
    "- **Query 3:** shows that we have a good distribution of entity types: Entity, Person, Location and Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping to Collect real-world data for our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_bbc_news(category=\"innovation\", num_articles=10):\n",
    "    \"\"\"\n",
    "    Scrape news articles from BBC News.\n",
    "    \n",
    "    Args:\n",
    "        category: News category (e.g., 'technology', 'business', 'science')\n",
    "        num_articles: Maximum number of articles to scrape\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing article data\n",
    "    \"\"\"\n",
    "    url = f\"https://www.bbc.com/{category}\"\n",
    "    \n",
    "    # Send request with headers to mimic a browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Scraping BBC News {category} section...\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = []\n",
    "        \n",
    "        # Find article links - this selector might need adjustment based on BBC's current layout\n",
    "        article_links = soup.select(\"a.gs-c-promo-heading\")\n",
    "        \n",
    "        if not article_links:\n",
    "            # Try alternative selectors if the primary one doesn't work\n",
    "            article_links = soup.select(\"a[data-component='link-tcf']\") or soup.select(\"a.ssrcss-rl2iw9-PromoLink\")\n",
    "        \n",
    "        print(f\"Found {len(article_links)} potential article links\")\n",
    "        \n",
    "        count = 0\n",
    "        for link in article_links:\n",
    "            if count >= num_articles:\n",
    "                break\n",
    "                \n",
    "            # Extract article URL\n",
    "            article_url = link.get('href')\n",
    "            if not article_url:\n",
    "                continue\n",
    "                \n",
    "            if not article_url.startswith('http'):\n",
    "                article_url = f\"https://www.bbc.com{article_url}\"\n",
    "            \n",
    "            # Extract article title\n",
    "            title = link.text.strip()\n",
    "            if not title:\n",
    "                title_elem = link.select_one(\"h3\") or link.select_one(\"span\")\n",
    "                title = title_elem.text.strip() if title_elem else \"Untitled Article\"\n",
    "            \n",
    "            # Skip non-article links\n",
    "            if not title or 'BBC' in title or len(title) < 10:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing article: {title}\")\n",
    "            \n",
    "            try:\n",
    "                # Get the full article page\n",
    "                article_response = requests.get(article_url, headers=headers)\n",
    "                article_response.raise_for_status()\n",
    "                \n",
    "                article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "                \n",
    "                # Extract article content - try different selectors for BBC's layout\n",
    "                content_blocks = article_soup.select(\"div[data-component='text-block']\")\n",
    "                \n",
    "                if not content_blocks:\n",
    "                    # Try alternative selectors\n",
    "                    content_blocks = article_soup.select(\"p.ssrcss-1q0x1qg-Paragraph\") or article_soup.select(\"article p\")\n",
    "                \n",
    "                content = \" \".join([block.text.strip() for block in content_blocks])\n",
    "                \n",
    "                # Get publication date\n",
    "                time_element = article_soup.select_one(\"time\")\n",
    "                publication_date = time_element.get('datetime') if time_element else \"Unknown\"\n",
    "                \n",
    "                # Add article to the list if we have meaningful content\n",
    "                if content and len(content) > 200:  # Minimum content length to ensure it's a real article\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': article_url,\n",
    "                        'content': content,\n",
    "                        'publication_date': publication_date\n",
    "                    })\n",
    "                    count += 1\n",
    "                    print(f\"Added article #{count}: {title}\")\n",
    "                    \n",
    "                # Be respectful with rate limiting\n",
    "                time.sleep(random.uniform(1.0, 2.0))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping article {article_url}: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping BBC News: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_theguardian_news(category=\"technology\", num_articles=10):\n",
    "    \"\"\"\n",
    "    Scrape news articles from The Guardian.\n",
    "    \n",
    "    Args:\n",
    "        category: News category (e.g., 'technology', 'business', 'science')\n",
    "        num_articles: Maximum number of articles to scrape\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing article data\n",
    "    \"\"\"\n",
    "    url = f\"https://www.theguardian.com/{category}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Scraping The Guardian {category} section...\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = []\n",
    "        \n",
    "        # Find article links\n",
    "        article_links = soup.select(\"a.dcr-lv2v9o\")\n",
    "        \n",
    "        print(f\"Found {len(article_links)} potential article links\")\n",
    "        \n",
    "        count = 0\n",
    "        for link in article_links:\n",
    "            if count >= num_articles:\n",
    "                break\n",
    "                \n",
    "            article_url = link.get('href')\n",
    "            if not article_url:\n",
    "                continue\n",
    "                \n",
    "            title = link.text.strip()\n",
    "            \n",
    "            # Skip non-article links\n",
    "            if not title or len(title) < 10:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing article: {title}\")\n",
    "            \n",
    "            try:\n",
    "                # Get the full article page\n",
    "                article_response = requests.get(article_url, headers=headers)\n",
    "                article_response.raise_for_status()\n",
    "                \n",
    "                article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "                \n",
    "                # Extract article content\n",
    "                content_blocks = article_soup.select(\"div.dcr-185kcx9 p\")\n",
    "                content = \" \".join([block.text.strip() for block in content_blocks])\n",
    "                \n",
    "                # Get publication date\n",
    "                time_element = article_soup.select_one(\"time\")\n",
    "                publication_date = time_element.get('datetime') if time_element else \"Unknown\"\n",
    "                \n",
    "                # Add article to the list\n",
    "                if content and len(content) > 200:\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': article_url,\n",
    "                        'content': content,\n",
    "                        'publication_date': publication_date\n",
    "                    })\n",
    "                    count += 1\n",
    "                    print(f\"Added article #{count}: {title}\")\n",
    "                    \n",
    "                time.sleep(random.uniform(1.0, 2.0))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping article {article_url}: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping The Guardian: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting web scraping...\n",
      "Scraping BBC News innovation section...\n",
      "Found 0 potential article links\n",
      "BBC scraping failed, trying The Guardian instead...\n",
      "Scraping The Guardian technology section...\n",
      "Found 0 potential article links\n",
      "\n",
      "Scraped 0 articles\n",
      "No articles were successfully scraped. Please check your internet connection or try a different website.\n"
     ]
    }
   ],
   "source": [
    "# Try to scrape articles from BBC News\n",
    "print(\"\\nStarting web scraping...\")\n",
    "articles = scrape_bbc_news(category=\"innovation\", num_articles=5)\n",
    "\n",
    "# If BBC scraping failed, try The Guardian as a backup\n",
    "if not articles:\n",
    "    print(\"BBC scraping failed, trying The Guardian instead...\")\n",
    "    articles = scrape_theguardian_news(category=\"technology\", num_articles=5)\n",
    "\n",
    "# Print information about scraped articles\n",
    "print(f\"\\nScraped {len(articles)} articles\")\n",
    "\n",
    "if articles:\n",
    "    for i, article in enumerate(articles):\n",
    "        print(f\"\\nArticle {i+1}: {article['title']}\")\n",
    "        print(f\"Date: {article['publication_date']}\")\n",
    "        print(f\"URL: {article['url']}\")\n",
    "        print(f\"Content Preview: {article['content'][:150]}...\")\n",
    "        \n",
    "    # Save articles to a file for backup\n",
    "    import json\n",
    "    with open(\"scraped_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\nArticles saved to 'scraped_articles.json'\")\n",
    "else:\n",
    "    print(\"No articles were successfully scraped. Please check your internet connection or try a different website.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Knowledge Graph Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_graph_pipeline(articles):\n",
    "    \"\"\"\n",
    "    Process a list of articles and build a combined knowledge graph.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of dictionaries containing article data\n",
    "        \n",
    "    Returns:\n",
    "        An RDFLib Graph object containing all triples from the articles\n",
    "    \"\"\"\n",
    "    # Create a new RDF graph\n",
    "    combined_graph = Graph()\n",
    "    EX = Namespace(\"http://example.org/\")\n",
    "    combined_graph.bind(\"ex\", EX)\n",
    "    \n",
    "    # Process each article\n",
    "    all_entities = []\n",
    "    all_relations = []\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        print(f\"\\nProcessing article {i+1}/{len(articles)}: {article['title']}\")\n",
    "        \n",
    "        # Process article content\n",
    "        content = article['content']\n",
    "        \n",
    "        # Build a knowledge graph for this article\n",
    "        article_graph, entities, relations = build_knowledge_graph(content)\n",
    "        all_entities.extend(entities)\n",
    "        all_relations.extend(relations)\n",
    "        \n",
    "        # Add article metadata to the graph\n",
    "        article_uri = URIRef(EX[f\"article_{i+1}\"])\n",
    "        combined_graph.add((article_uri, RDF.type, EX.Article))\n",
    "        combined_graph.add((article_uri, RDFS.label, Literal(article['title'])))\n",
    "        combined_graph.add((article_uri, EX.url, Literal(article['url'])))\n",
    "        combined_graph.add((article_uri, EX.publishedDate, Literal(article['publication_date'])))\n",
    "        \n",
    "        # Add all triples from the article graph to the combined graph\n",
    "        for s, p, o in article_graph:\n",
    "            combined_graph.add((s, p, o))\n",
    "            \n",
    "            # Link entities to the article they were mentioned in\n",
    "            if isinstance(s, URIRef) and not str(s).endswith(f\"article_{i+1}\"):\n",
    "                combined_graph.add((s, EX.mentionedIn, article_uri))\n",
    "    \n",
    "    # Print statistics\n",
    "    entity_types = {}\n",
    "    for _, entity_type, _, _ in all_entities:\n",
    "        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "        \n",
    "    print(\"\\nKnowledge Graph Statistics:\")\n",
    "    print(f\"Total entities: {len(all_entities)}\")\n",
    "    print(\"Entity types:\")\n",
    "    for entity_type, count in entity_types.items():\n",
    "        print(f\"  {entity_type}: {count}\")\n",
    "    \n",
    "    print(f\"Total relations: {len(all_relations)}\")\n",
    "    print(f\"Total triples in graph: {len(combined_graph)}\")\n",
    "    \n",
    "    return combined_graph, all_entities, all_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have scraped articles\n",
    "if 'articles' in locals() and articles:\n",
    "    # Run the complete pipeline\n",
    "    print(\"\\nRunning the complete knowledge graph pipeline...\")\n",
    "    knowledge_graph, all_entities, all_relations = knowledge_graph_pipeline(articles)\n",
    "    \n",
    "    # Save the knowledge graph to a file\n",
    "    knowledge_graph.serialize(destination=\"news_knowledge_graph.ttl\", format=\"turtle\")\n",
    "    print(\"\\nKnowledge graph saved to 'news_knowledge_graph.ttl'\")\n",
    "    \n",
    "    # Perform interesting queries\n",
    "    print(\"\\nPerforming SPARQL queries on the news knowledge graph:\")\n",
    "    \n",
    "    # Query 1: Find the most frequently mentioned entities\n",
    "    query1 = \"\"\"\n",
    "    SELECT ?entity ?label (COUNT(?article) as ?mentions)\n",
    "    WHERE {\n",
    "        ?entity <http://example.org/mentionedIn> ?article .\n",
    "        ?entity <http://www.w3.org/2000/01/rdf-schema#label> ?label .\n",
    "    }\n",
    "    GROUP BY ?entity ?label\n",
    "    ORDER BY DESC(?mentions)\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 1: Top 10 most frequently mentioned entities\")\n",
    "    results1 = list(knowledge_graph.query(query1))\n",
    "    if results1:\n",
    "        for row in results1:\n",
    "            print(f\"{row.label}: {row.mentions} mentions\")\n",
    "    else:\n",
    "        print(\"No entity mentions found\")\n",
    "    \n",
    "    # Query 2: Find entities by type\n",
    "    query2 = \"\"\"\n",
    "    SELECT ?type (COUNT(?entity) as ?count)\n",
    "    WHERE {\n",
    "        ?entity a ?type .\n",
    "    }\n",
    "    GROUP BY ?type\n",
    "    ORDER BY DESC(?count)\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 2: Entity counts by type\")\n",
    "    for row in knowledge_graph.query(query2):\n",
    "        type_name = str(row.type).split('/')[-1]\n",
    "        print(f\"{type_name}: {row.count}\")\n",
    "    \n",
    "    # Query 3: Find relationships between entities\n",
    "    query3 = \"\"\"\n",
    "    SELECT ?s_label ?p ?o_label\n",
    "    WHERE {\n",
    "        ?s ?p ?o .\n",
    "        ?s <http://www.w3.org/2000/01/rdf-schema#label> ?s_label .\n",
    "        ?o <http://www.w3.org/2000/01/rdf-schema#label> ?o_label .\n",
    "        FILTER(?p != <http://www.w3.org/2000/01/rdf-schema#label>)\n",
    "        FILTER(?p != <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)\n",
    "        FILTER(?p != <http://example.org/mentionedIn>)\n",
    "    }\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 3: Sample relationships between entities\")\n",
    "    results3 = list(knowledge_graph.query(query3))\n",
    "    if results3:\n",
    "        for row in results3:\n",
    "            p_name = str(row.p).split('/')[-1]\n",
    "            print(f\"{row.s_label} --{p_name}--> {row.o_label}\")\n",
    "    else:\n",
    "        print(\"No relationships found\")\n",
    "    \n",
    "    # Query 4: Find co-occurring entities (entities mentioned in the same article)\n",
    "    query4 = \"\"\"\n",
    "    SELECT ?entity1_label ?entity2_label (COUNT(?article) as ?co_occurrences)\n",
    "    WHERE {\n",
    "        ?entity1 <http://example.org/mentionedIn> ?article .\n",
    "        ?entity2 <http://example.org/mentionedIn> ?article .\n",
    "        ?entity1 <http://www.w3.org/2000/01/rdf-schema#label> ?entity1_label .\n",
    "        ?entity2 <http://www.w3.org/2000/01/rdf-schema#label> ?entity2_label .\n",
    "        FILTER(?entity1 != ?entity2)\n",
    "        FILTER(?entity1_label < ?entity2_label)  # To avoid duplicates\n",
    "    }\n",
    "    GROUP BY ?entity1_label ?entity2_label\n",
    "    ORDER BY DESC(?co_occurrences)\n",
    "    LIMIT 15\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 4: Top entity co-occurrences\")\n",
    "    results4 = list(knowledge_graph.query(query4))\n",
    "    if results4:\n",
    "        for row in results4:\n",
    "            print(f\"{row.entity1_label} & {row.entity2_label}: {row.co_occurrences} articles\")\n",
    "    else:\n",
    "        print(\"No co-occurring entities found\")\n",
    "    \n",
    "    # Query 5: Find the most mentioned locations\n",
    "    query5 = \"\"\"\n",
    "    SELECT ?location ?label (COUNT(?article) as ?mentions)\n",
    "    WHERE {\n",
    "        ?location a <http://example.org/Location> .\n",
    "        ?location <http://example.org/mentionedIn> ?article .\n",
    "        ?location <http://www.w3.org/2000/01/rdf-schema#label> ?label .\n",
    "    }\n",
    "    GROUP BY ?location ?label\n",
    "    ORDER BY DESC(?mentions)\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 5: Top 10 most mentioned locations\")\n",
    "    results5 = list(knowledge_graph.query(query5))\n",
    "    if results5:\n",
    "        for row in results5:\n",
    "            print(f\"{row.label}: {row.mentions} mentions\")\n",
    "    else:\n",
    "        print(\"No location mentions found\")\n",
    "    \n",
    "    # Additional analytics: Calculate the density of the knowledge graph\n",
    "    num_entities = len(set([s for s, p, o in knowledge_graph if isinstance(s, URIRef) and p == RDF.type]))\n",
    "    num_relations = len(knowledge_graph) - num_entities * 2  # Subtract type and label triples\n",
    "    max_possible_relations = num_entities * (num_entities - 1)\n",
    "    density = 0 if max_possible_relations == 0 else num_relations / max_possible_relations\n",
    "    \n",
    "    print(\"\\nKnowledge Graph Analysis:\")\n",
    "    print(f\"Number of entities: {num_entities}\")\n",
    "    print(f\"Number of relationships: {num_relations}\")\n",
    "    print(f\"Graph density: {density:.6f}\")\n",
    "    \n",
    "    # Generate a summary of the knowledge graph\n",
    "    print(\"\\nKnowledge Graph Summary:\")\n",
    "    print(f\"The knowledge graph was built from {len(articles)} news articles.\")\n",
    "    print(f\"It contains {num_entities} entities and {num_relations} relationships.\")\n",
    "    print(\"The most common entity types are:\")\n",
    "    for row in list(knowledge_graph.query(query2))[:3]:\n",
    "        type_name = str(row.type).split('/')[-1]\n",
    "        print(f\"  - {type_name}: {row.count} entities\")\n",
    "    \n",
    "    print(\"\\nThe most frequently mentioned entities are:\")\n",
    "    for row in results1[:5]:\n",
    "        print(f\"  - {row.label}: {row.mentions} mentions\")\n",
    "    \n",
    "    print(\"\\nThis knowledge graph can be used for entity-centric news analysis, tracking relationships between organizations, people, and locations, and identifying trending topics in the news.\")\n",
    "else:\n",
    "    print(\"Cannot build knowledge graph without articles. Please run the web scraping step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this does is:\n",
    "1. Processes all scraped articles though out entity and relation extraction pipeline\n",
    "2. Combines them into a unified knowledge graph\n",
    "3. Adds metadata about the articles, inluding links from entitites to the articles they appear in\n",
    "4. Runs comprehensive SPARQL queries to analyze the graph\n",
    "5. Calculates graph statistics like density and entity type distribution\n",
    "6. Provides a summary of the knowledge graph's contents and potential applications\n",
    "\n",
    "The queries are designed to extract useful insights:\n",
    "- most frequently mentioned entities\n",
    "- entitiy counts by type\n",
    "- sample relationships between entities\n",
    "- co-occuring entities (which could suggest hidden relationships)\n",
    "- top mentioned locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "We have:\n",
    "1. Setup our environment: We setup the necessary libraries and dependencies\n",
    "2. Cleaned / preprocessed the text: We implemented text cleaning functions\n",
    "3. NER: We compared spaCy's pretrained NER model with a CRF model\n",
    "4. Relation extraction: We implemented relation extraction using spaCy's dependency parsing\n",
    "5. Knowledge graph building: We constructed an RDF graph and ran SPARQL queries on it\n",
    "6. Webscraping: We implemented functions to scrape news articles from 2 websites (still doesn't work)\n",
    "7. Complete pipeline: We combined all the above components into a unified workflow\n",
    "\n",
    "For our project report we should include:\n",
    "1. A description of our methodology for each step\n",
    "2. The challenges we faced (like the CRF model implementation and relation extraction patterns)\n",
    "3. Sample queries and results from our knowledge graph\n",
    "4. Examples of entities and relations extacted from our scraped articles\n",
    "5. Potential applications and future improvements for our knowledge ggraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Challenge with Relation Extraction\n",
    "Initially, our relation extraction approach was quite limited. We were primarily looking for simple subject-verb-object patterns in the dependency parse tree, but this wasn't capturing many of the actual relationships in the text. When we tested it on our sample sentences, we got \"No relations found\" for all of them.\n",
    "\n",
    "\n",
    "#### The Improvements We Made\n",
    "We enhanced our relation extraction function in several key ways:\n",
    "1. **Entity-Focused Approach** : Rather than trying to parse general sentence structure, we shifted to specifically looking for relationships between named entities that spaCy had already identified. This focused our extraction on meaningful relations between important entities rather than trying to extract all possible sentence relationships.\n",
    "2. **Multiple Pattern Recognition** : We added several additional patterns beyond the basic subject-verb-object:\n",
    "   - Direct subject-verb-object patterns: Captures basic statements like \"Apple was founded by Steve Jobs\"\n",
    "   - Entity-verb-prep-Entity patterns: Captures statements like \"Amazon has headquarters in Seattle\"\n",
    "   - Adjacent entity relationships: Identifies relationships between entities that appear close to each other, such as \"Elon Musk, CEO of Tesla\"\n",
    "3. **Tracking Entity Provenance** : We maintained a mapping between entity tokens and their full entity mentions, which helped us properly reconstruct the relationships between the full entity names rather than just the individual tokens.\n",
    "4. **Sentence-Level Processing** : We processed text sentence by sentence, which helped maintain the correct contextual relationships and prevented incorrect connections across sentence boundaries.\n",
    "5. **Flexible Relationship Types** : When we couldn't determine a specific relationship type from the text, we used a generic \"related_to\" predicate. This allowed us to still capture the relationship between entities even when the exact nature was ambiguous.\n",
    "\n",
    "#### The Results:\n",
    "After implementing these improvements, our relation extraction started successfully identifying meaningful relationships:\n",
    "\n",
    "- We captured \"Elon Musk (PERSON) --related_to--> Texas (GPE)\" from \"Tesla and SpaceX are companies founded by Elon Musk who lives in Texas.\"\n",
    "- We identified \"Amazon (ORG) --related_to--> Seattle (GPE)\" from \"Amazon has its headquarters in Seattle, Washington.\"\n",
    "\n",
    "These improvements turned the relation extraction from a non-functioning component to a key part of our knowledge graph pipeline. It allowed us to populate our graph with meaningful connections between entities, rather than just having isolated entity nodes.\n",
    "\n",
    "#### Limitations and Future Improvements\n",
    "While our approach improved greatly, it still has limitations:\n",
    "1. **Relationship Specificity** : Many relationships are tagged with the generic \"related_to\" predicate rather than more specific relationships.\n",
    "2. **Complex Relationship Structures** : Our current patterns don't handle more complex linguistic structures like coreference (when a pronoun refers to a previously mentioned entity) or nested relationships.\n",
    "3. **Context Sensitivity** : The current extraction doesn't consider broader document context that might clarify relationships.\n",
    "\n",
    "To further improve the relation extraction, we coould:\n",
    "1. Train a supervised relation extraction model on labeled data\n",
    "2. Implement more sophisticated linguistic patterns\n",
    "3. Add coreference resolution to connect pronouns to their referenced entities\n",
    "4. Use pre-trained models specifically designed for relation extraction\n",
    "\n",
    "Despite these limitations, our improved relation extraction approach successfully extracts meaningful relationships that form the backbone of our knowledge graph, which was our primary goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
