{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Verifying that all libraries are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are installed!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "required_packages =[\n",
    "    'beautifulsoup4', 'spacy', 'nltk', 'sklearn-crfsuite', 'datasets', 'transformers', 'rdflib', 'requests'\n",
    "]\n",
    "\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = {pkg for pkg in required_packages if pkg.lower() not in installed}\n",
    "\n",
    "if missing:\n",
    "    print(f\"Missing packages: {missing}\")\n",
    "    # pip install {' '.join(missing)}\n",
    "\n",
    "else: \n",
    "    print(\"All required packages are installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading necessary NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading spacy models...\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading spacy models...\")\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rayanhamadeh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Basic text cleaning\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove numbers but keep hyphenated words\n",
    "    text = re.sub(r'(?<!\\w)[-+]?\\d+', '', text)\n",
    "\n",
    "    # Keep hyphens for compound words but remove other punctuation\n",
    "    text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    More advanced preprocessing with tokenization, stopword removal, and lemmatization \n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text first\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join back to text\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Apple Inc. was founded by Steve Jobs in 1976. Their headquarters is in Cupertino, California. The CEO is Tim Cook.\n",
      "Cleaned: apple inc was founded by steve jobs in their headquarters is in cupertino california the ceo is tim cook\n",
      "Preprocessed: apple inc founded steve job headquarters cupertino california ceo tim cook\n"
     ]
    }
   ],
   "source": [
    "# Testing the functions\n",
    "sample_text = \"Apple Inc. was founded by Steve Jobs in 1976. Their headquarters is in Cupertino, California. The CEO is Tim Cook.\"\n",
    "print (\"Original:\", sample_text)\n",
    "print (\"Cleaned:\", clean_text(sample_text))\n",
    "print (\"Preprocessed:\", preprocess_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entry Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayanhamadeh/anaconda3/envs/TF/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CoNLL-2003 dataset...\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading CoNLL-2003 dataset...\")\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extraction functions for CRF\n",
    "def sent2features(sentence_data):\n",
    "    \"\"\"Extract features for each word in the sentence.\"\"\"\n",
    "    tokens = sentence_data['tokens']\n",
    "    pos_tags = sentence_data['pos_tags']\n",
    "    \n",
    "    features = []\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        postag = str(pos_tags[i])  # Convert to string to be safe\n",
    "        \n",
    "        word_features = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word[-3:]': word[-3:] if len(word) > 3 else word,\n",
    "            'word[-2:]': word[-2:] if len(word) > 2 else word,\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'postag': postag,\n",
    "        }\n",
    "        \n",
    "        # Features for words that are not at the beginning of a document\n",
    "        if i > 0:\n",
    "            word1 = tokens[i-1]\n",
    "            postag1 = str(pos_tags[i-1])\n",
    "            word_features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:postag': postag1,\n",
    "            })\n",
    "        else:\n",
    "            # Indicate that this is the beginning of a document\n",
    "            word_features['BOS'] = True\n",
    "        \n",
    "        # Features for words that are not at the end of a document\n",
    "        if i < len(tokens) - 1:\n",
    "            word1 = tokens[i+1]\n",
    "            postag1 = str(pos_tags[i+1])\n",
    "            word_features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:postag': postag1,\n",
    "            })\n",
    "        else:\n",
    "            # Indicate that this is the end of a document\n",
    "            word_features['EOS'] = True\n",
    "        \n",
    "        features.append(word_features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sentence_data):\n",
    "    \"\"\"Extract NER labels for each word in the sentence.\"\"\"\n",
    "    return [str(label) for label in sentence_data['ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for CRF model...\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for CRF\n",
    "print(\"Preparing data for CRF model...\")\n",
    "# Using smaller subsets for faster training and evaluation\n",
    "X_train = [sent2features(sentence) for sentence in train_dataset.select(range(1000))]\n",
    "y_train = [sent2labels(sentence) for sentence in train_dataset.select(range(1000))]\n",
    "\n",
    "X_test = [sent2features(sentence) for sentence in test_dataset.select(range(100))]\n",
    "y_test = [sent2labels(sentence) for sentence in test_dataset.select(range(100))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CRF model (this might take a while)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=False, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=False, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=False, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train CRF model\n",
    "print(\"Training CRF model (this might take a while)...\")\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CRF model...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1086\n",
      "           1       0.94      0.85      0.89       111\n",
      "           2       0.92      0.98      0.95        97\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.84      0.79      0.82        82\n",
      "           6       0.50      0.22      0.31         9\n",
      "           7       0.95      0.86      0.90        22\n",
      "           8       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.96      1422\n",
      "   macro avg       0.67      0.62      0.64      1422\n",
      "weighted avg       0.96      0.96      0.96      1422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CRF model\n",
    "print(\"Evaluating CRF model...\")\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map CoNLL-2003 numeric labels to text labels\n",
    "def map_conll_labels(label_id):\n",
    "    \"\"\"Map CoNLL-2003 numeric labels to text labels.\"\"\"\n",
    "    label_map = {\n",
    "        '0': 'O',       # Outside of a named entity\n",
    "        '1': 'B-PER',   # Beginning of person name\n",
    "        '2': 'I-PER',   # Inside of person name\n",
    "        '3': 'B-ORG',   # Beginning of organization name\n",
    "        '4': 'I-ORG',   # Inside of organization name\n",
    "        '5': 'B-LOC',   # Beginning of location name\n",
    "        '6': 'I-LOC',   # Inside of location name\n",
    "        '7': 'B-MISC',  # Beginning of miscellaneous entity\n",
    "        '8': 'I-MISC'   # Inside of miscellaneous entity\n",
    "    }\n",
    "    return label_map.get(label_id, label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy NER model...\n",
      "The spaCy en_ner_conll03 model isn't installed. Using en_core_web_sm instead.\n",
      "Successfully loaded en_core_web_sm model\n"
     ]
    }
   ],
   "source": [
    "# Using spaCy's pre-trained NER model\n",
    "print(\"Loading spaCy NER model...\")\n",
    "try:\n",
    "    nlp_ner = spacy.load(\"en_ner_conll03\")\n",
    "    print(\"Successfully loaded en_ner_conll03 model\")\n",
    "except OSError:\n",
    "    print(\"The spaCy en_ner_conll03 model isn't installed. Using en_core_web_sm instead.\")\n",
    "    try:\n",
    "        nlp_ner = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"Successfully loaded en_core_web_sm model\")\n",
    "    except OSError:\n",
    "        print(\"Downloading en_core_web_sm model...\")\n",
    "        spacy.cli.download(\"en_core_web_sm\")\n",
    "        nlp_ner = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_spacy(text):\n",
    "    \"\"\"Extract named entities using spaCy.\"\"\"\n",
    "    doc = nlp_ner(text)\n",
    "    entities = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities_crf(text, crf_model):\n",
    "    \"\"\"\n",
    "    Predict named entities in text using trained CRF model.\n",
    "    \"\"\"\n",
    "    # Tokenize text (simple split for basic demonstration)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Use spaCy for better tokenization and POS tagging\n",
    "    doc = nlp_ner(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Create features that match our training features more closely\n",
    "    features = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_features = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower(),\n",
    "            'word[-3:]': token[-3:] if len(token) > 3 else token,\n",
    "            'word[-2:]': token[-2:] if len(token) > 2 else token,\n",
    "            'word.isupper()': token.isupper(),\n",
    "            'word.istitle()': token.istitle(),\n",
    "            'word.isdigit()': token.isdigit(),\n",
    "            'postag': pos_tags[i],\n",
    "        }\n",
    "        \n",
    "        # Add features for previous word if not at beginning\n",
    "        if i > 0:\n",
    "            token_features.update({\n",
    "                '-1:word.lower()': tokens[i-1].lower(),\n",
    "                '-1:word.istitle()': tokens[i-1].istitle(),\n",
    "                '-1:word.isupper()': tokens[i-1].isupper(),\n",
    "                '-1:postag': pos_tags[i-1],\n",
    "            })\n",
    "        else:\n",
    "            token_features['BOS'] = True\n",
    "            \n",
    "        # Add features for next word if not at end\n",
    "        if i < len(tokens) - 1:\n",
    "            token_features.update({\n",
    "                '+1:word.lower()': tokens[i+1].lower(),\n",
    "                '+1:word.istitle()': tokens[i+1].istitle(),\n",
    "                '+1:word.isupper()': tokens[i+1].isupper(),\n",
    "                '+1:postag': pos_tags[i+1],\n",
    "            })\n",
    "        else:\n",
    "            token_features['EOS'] = True\n",
    "            \n",
    "        features.append(token_features)\n",
    "    \n",
    "    # Make prediction\n",
    "    if features:  # Check if we have any features to predict on\n",
    "        predictions = crf_model.predict([features])[0]\n",
    "        \n",
    "        # For debugging\n",
    "        print(\"DEBUG - Tokens:\", tokens)\n",
    "        print(\"DEBUG - Predictions:\", predictions)\n",
    "        \n",
    "        # Combine tokens with their predicted labels\n",
    "        entities = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if predictions[i].startswith('B-') or (predictions[i] != '0' and predictions[i] != 'O'):\n",
    "                # Handle both string labels and numeric indices from CoNLL\n",
    "                entity_type = predictions[i][2:] if predictions[i].startswith('B-') else map_conll_labels(predictions[i])\n",
    "                entity_text = tokens[i]\n",
    "                j = i + 1\n",
    "                \n",
    "                # Continue collecting tokens that are part of the same entity\n",
    "                while j < len(tokens) and (predictions[j].startswith('I-') or predictions[j] == str(int(predictions[i]) + 1)):\n",
    "                    entity_text += ' ' + tokens[j]\n",
    "                    j += 1\n",
    "                \n",
    "                # Add the entity to our list\n",
    "                start_pos = text.find(tokens[i])\n",
    "                end_pos = start_pos + len(entity_text)\n",
    "                entities.append((entity_text, entity_type, start_pos, end_pos))\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return entities\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text: Apple was founded by Steve Jobs. The company is headquartered in Cupertino, California.\n",
      "\n",
      "spaCy NER results:\n",
      "Entity: Apple, Type: ORG, Position: 0-5\n",
      "Entity: Steve Jobs, Type: PERSON, Position: 21-31\n",
      "Entity: Cupertino, Type: GPE, Position: 65-74\n",
      "Entity: California, Type: GPE, Position: 76-86\n",
      "\n",
      "CRF model results:\n",
      "DEBUG - Tokens: ['Apple', 'was', 'founded', 'by', 'Steve', 'Jobs', '.', 'The', 'company', 'is', 'headquartered', 'in', 'Cupertino', ',', 'California', '.']\n",
      "DEBUG - Predictions: ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '5' '0']\n",
      "Entity: California, Type: B-LOC, Position: 76-86\n"
     ]
    }
   ],
   "source": [
    "# Compare models on a sample text\n",
    "sample_text = \"Apple was founded by Steve Jobs. The company is headquartered in Cupertino, California.\"\n",
    "print(\"\\nSample text:\", sample_text)\n",
    "\n",
    "print(\"\\nspaCy NER results:\")\n",
    "spacy_entities = extract_entities_spacy(sample_text)\n",
    "for entity in spacy_entities:\n",
    "    print(f\"Entity: {entity[0]}, Type: {entity[1]}, Position: {entity[2]}-{entity[3]}\")\n",
    "\n",
    "print(\"\\nCRF model results:\")\n",
    "crf_entities = predict_entities_crf(sample_text, crf)\n",
    "for entity in crf_entities:\n",
    "    print(f\"Entity: {entity[0]}, Type: {entity[1]}, Position: {entity[2]}-{entity[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing more samples:\n",
      "\n",
      "Sample 1: Microsoft CEO Satya Nadella announced new products at the conference in Seattle.\n",
      "spaCy entities:\n",
      "  Microsoft (ORG)\n",
      "  Satya Nadella (PERSON)\n",
      "  Seattle (GPE)\n",
      "CRF entities:\n",
      "DEBUG - Tokens: ['Microsoft', 'CEO', 'Satya', 'Nadella', 'announced', 'new', 'products', 'at', 'the', 'conference', 'in', 'Seattle', '.']\n",
      "DEBUG - Predictions: ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      "\n",
      "Sample 2: Tesla and SpaceX are companies founded by Elon Musk who lives in Texas.\n",
      "spaCy entities:\n",
      "  Elon Musk (PERSON)\n",
      "  Texas (GPE)\n",
      "CRF entities:\n",
      "DEBUG - Tokens: ['Tesla', 'and', 'SpaceX', 'are', 'companies', 'founded', 'by', 'Elon', 'Musk', 'who', 'lives', 'in', 'Texas', '.']\n",
      "DEBUG - Predictions: ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      "\n",
      "Sample 3: Amazon has its headquarters in Seattle, Washington, which is a city in the United States.\n",
      "spaCy entities:\n",
      "  Amazon (ORG)\n",
      "  Seattle (GPE)\n",
      "  Washington (GPE)\n",
      "  the United States (GPE)\n",
      "CRF entities:\n",
      "DEBUG - Tokens: ['Amazon', 'has', 'its', 'headquarters', 'in', 'Seattle', ',', 'Washington', ',', 'which', 'is', 'a', 'city', 'in', 'the', 'United', 'States', '.']\n",
      "DEBUG - Predictions: ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "# Evaluate further sample texts\n",
    "print(\"\\nTesting more samples:\")\n",
    "test_samples = [\n",
    "    \"Microsoft CEO Satya Nadella announced new products at the conference in Seattle.\",\n",
    "    \"Tesla and SpaceX are companies founded by Elon Musk who lives in Texas.\",\n",
    "    \"Amazon has its headquarters in Seattle, Washington, which is a city in the United States.\"\n",
    "]\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"\\nSample {i+1}: {sample}\")\n",
    "    print(\"spaCy entities:\")\n",
    "    for entity in extract_entities_spacy(sample):\n",
    "        print(f\"  {entity[0]} ({entity[1]})\")\n",
    "    \n",
    "    print(\"CRF entities:\")\n",
    "    for entity in predict_entities_crf(sample, crf):\n",
    "        print(f\"  {entity[0]} ({entity[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully implemented both NER approaches but the spaCy model outperformed the CRF model out-of-the-box, this is likely due to its extensive pretraining.\n",
    "\n",
    "The CRF model would likely require more data and tuning to acheive comparable results.\n",
    "\n",
    "For practicality we decided to continue with spaCy for the remainder of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation Extraction with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations(text):\n",
    "    \"\"\"Extract relations between entities using a more comprehensive approach.\"\"\"\n",
    "    doc = nlp_ner(text)\n",
    "    relations = []\n",
    "    \n",
    "    # Get all named entities\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    entity_tokens = {}\n",
    "    \n",
    "    # Map entity texts to their tokens\n",
    "    for ent in doc.ents:\n",
    "        for token in ent:\n",
    "            entity_tokens[token.i] = (ent.text, ent.label_)\n",
    "    \n",
    "    # Find subject-verb-object patterns\n",
    "    for sent in doc.sents:\n",
    "        # First, get all potential subjects (entities that are subjects in the sentence)\n",
    "        subjects = []\n",
    "        for token in sent:\n",
    "            # Check if token is a subject and part of a named entity\n",
    "            if token.dep_ in (\"nsubj\", \"nsubjpass\") and token.i in entity_tokens:\n",
    "                subjects.append((token, entity_tokens[token.i]))\n",
    "        \n",
    "        # For each potential subject\n",
    "        for subj_token, subj_entity in subjects:\n",
    "            verb = subj_token.head  # The verb is the head of the subject\n",
    "            \n",
    "            # Look for direct objects\n",
    "            for token in verb.children:\n",
    "                # Direct object\n",
    "                if token.dep_ in (\"dobj\", \"attr\", \"pobj\") and token.i in entity_tokens:\n",
    "                    obj_entity = entity_tokens[token.i]\n",
    "                    relations.append({\n",
    "                        \"subject\": subj_entity[0],\n",
    "                        \"subject_type\": subj_entity[1],\n",
    "                        \"predicate\": verb.text,\n",
    "                        \"object\": obj_entity[0],\n",
    "                        \"object_type\": obj_entity[1]\n",
    "                    })\n",
    "                \n",
    "                # Prepositional phrase\n",
    "                elif token.dep_ == \"prep\":\n",
    "                    prep = token\n",
    "                    for pobj in prep.children:\n",
    "                        if pobj.dep_ == \"pobj\" and pobj.i in entity_tokens:\n",
    "                            obj_entity = entity_tokens[pobj.i]\n",
    "                            relations.append({\n",
    "                                \"subject\": subj_entity[0],\n",
    "                                \"subject_type\": subj_entity[1],\n",
    "                                \"predicate\": f\"{verb.text} {prep.text}\",\n",
    "                                \"object\": obj_entity[0],\n",
    "                                \"object_type\": obj_entity[1]\n",
    "                            })\n",
    "    \n",
    "    for ent1 in doc.ents:\n",
    "        for ent2 in doc.ents:\n",
    "            if ent1.text != ent2.text:  # Don't relate an entity to itself\n",
    "                # Find a path connecting these entities\n",
    "                for token in ent1:\n",
    "                    # Look at verbs connected to this entity\n",
    "                    if token.head.pos_ == \"VERB\":\n",
    "                        verb = token.head\n",
    "                        # Look for prepositions in the verb's children\n",
    "                        for child in verb.children:\n",
    "                            if child.dep_ == \"prep\":\n",
    "                                prep = child\n",
    "                                # Check if this prep connects to the second entity\n",
    "                                for token2 in ent2:\n",
    "                                    if token2.head == prep:\n",
    "                                        relations.append({\n",
    "                                            \"subject\": ent1.text,\n",
    "                                            \"subject_type\": ent1.label_,\n",
    "                                            \"predicate\": f\"{verb.text} {prep.text}\",\n",
    "                                            \"object\": ent2.text,\n",
    "                                            \"object_type\": ent2.label_\n",
    "                                        })\n",
    "    \n",
    "    prev_ent = None\n",
    "    for ent in doc.ents:\n",
    "        if prev_ent:\n",
    "            # Check if they're close to each other\n",
    "            if ent.start - prev_ent.end <= 5:  # Within 5 tokens\n",
    "                # Check for specific patterns like apposition\n",
    "                for token in doc[prev_ent.start:ent.start]:\n",
    "                    if token.dep_ == \"appos\" or token.text in [\"of\", \"at\", \"in\", \"from\", \"by\"]:\n",
    "                        relations.append({\n",
    "                            \"subject\": prev_ent.text,\n",
    "                            \"subject_type\": prev_ent.label_,\n",
    "                            \"predicate\": \"related_to\",  # Generic relation\n",
    "                            \"object\": ent.text,\n",
    "                            \"object_type\": ent.label_\n",
    "                        })\n",
    "        prev_ent = ent\n",
    "    \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing relation extraction:\n",
      "\n",
      "Sample 1: Microsoft CEO Satya Nadella announced new products at the conference in Seattle.\n",
      "Entities:\n",
      "  Microsoft (ORG)\n",
      "  Satya Nadella (PERSON)\n",
      "  Seattle (GPE)\n",
      "Relations:\n",
      "  No relations found\n",
      "\n",
      "Sample 2: Tesla and SpaceX are companies founded by Elon Musk who lives in Texas.\n",
      "Entities:\n",
      "  Elon Musk (PERSON)\n",
      "  Texas (GPE)\n",
      "Relations:\n",
      "  Elon Musk (PERSON) --related_to--> Texas (GPE)\n",
      "\n",
      "Sample 3: Amazon has its headquarters in Seattle, Washington, which is a city in the United States.\n",
      "Entities:\n",
      "  Amazon (ORG)\n",
      "  Seattle (GPE)\n",
      "  Washington (GPE)\n",
      "  the United States (GPE)\n",
      "Relations:\n",
      "  Amazon (ORG) --related_to--> Seattle (GPE)\n"
     ]
    }
   ],
   "source": [
    "# Test the relation extraction on our sample texts\n",
    "print(\"\\nTesting relation extraction:\")\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"\\nSample {i+1}: {sample}\")\n",
    "    entities = extract_entities_spacy(sample)\n",
    "    print(\"Entities:\")\n",
    "    for entity in entities:\n",
    "        print(f\"  {entity[0]} ({entity[1]})\")\n",
    "    \n",
    "    relations = extract_relations(sample)\n",
    "    print(\"Relations:\")\n",
    "    if relations:\n",
    "        for relation in relations:\n",
    "            print(f\"  {relation['subject']} ({relation['subject_type']}) --{relation['predicate']}--> {relation['object']} ({relation['object_type']})\")\n",
    "    else:\n",
    "        print(\"  No relations found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Graph Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_uri(text):\n",
    "    \"\"\"Convert text to a valid URI component.\"\"\"\n",
    "    # Remove special characters and spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Replace spaces with underscores and convert to lowercase\n",
    "    return text.replace(' ', '_').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph(text):\n",
    "    \"\"\"\n",
    "    Build a knowledge graph from text using spaCy for entity and relation extraction.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to process\n",
    "        \n",
    "    Returns:\n",
    "        An RDFLib Graph object\n",
    "    \"\"\"\n",
    "    # Create a new RDF graph\n",
    "    g = Graph()\n",
    "    \n",
    "    # Define namespaces\n",
    "    EX = Namespace(\"http://example.org/\")\n",
    "    g.bind(\"ex\", EX)\n",
    "    \n",
    "    # Extract entities and relations\n",
    "    entities = extract_entities_spacy(text)\n",
    "    relations = extract_relations(text)\n",
    "    \n",
    "    # Process entities\n",
    "    entity_uris = {}\n",
    "    for entity_text, entity_type, start, end in entities:\n",
    "        # Create a URI for the entity\n",
    "        entity_uri = URIRef(EX[text_to_uri(entity_text)])\n",
    "        entity_uris[entity_text] = entity_uri\n",
    "        \n",
    "        # Add entity type information\n",
    "        if entity_type == \"PERSON\":\n",
    "            g.add((entity_uri, RDF.type, EX.Person))\n",
    "        elif entity_type == \"ORG\":\n",
    "            g.add((entity_uri, RDF.type, EX.Organization))\n",
    "        elif entity_type in [\"GPE\", \"LOC\"]:\n",
    "            g.add((entity_uri, RDF.type, EX.Location))\n",
    "        else:\n",
    "            g.add((entity_uri, RDF.type, EX.Entity))\n",
    "        \n",
    "        # Add entity label\n",
    "        g.add((entity_uri, RDFS.label, Literal(entity_text)))\n",
    "    \n",
    "    # Process relations\n",
    "    for relation in relations:\n",
    "        subject_text = relation[\"subject\"]\n",
    "        object_text = relation[\"object\"]\n",
    "        predicate = relation[\"predicate\"]\n",
    "        \n",
    "        # Get or create URIs for subject and object\n",
    "        if subject_text in entity_uris:\n",
    "            subject_uri = entity_uris[subject_text]\n",
    "        else:\n",
    "            # Create a URI if not already created\n",
    "            subject_uri = URIRef(EX[text_to_uri(subject_text)])\n",
    "            g.add((subject_uri, RDFS.label, Literal(subject_text)))\n",
    "        \n",
    "        if object_text in entity_uris:\n",
    "            object_uri = entity_uris[object_text]\n",
    "        else:\n",
    "            # Create a URI if not already created\n",
    "            object_uri = URIRef(EX[text_to_uri(object_text)])\n",
    "            g.add((object_uri, RDFS.label, Literal(object_text)))\n",
    "        \n",
    "        # Create a predicate URI\n",
    "        predicate_uri = URIRef(EX[text_to_uri(predicate)])\n",
    "        \n",
    "        # Add the relation triple\n",
    "        g.add((subject_uri, predicate_uri, object_uri))\n",
    "    \n",
    "    return g, entities, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Star Wars example text...\n",
      "\n",
      "Extracted entities from Star Wars text:\n",
      "Entity: Movie, Type: NORP\n",
      "Entity: Jedis, Type: PERSON\n",
      "Entity: Luke, Type: PERSON\n",
      "Entity: Jedi, Type: PERSON\n",
      "Entity: Master Yoda, Type: PERSON\n",
      "Entity: Jedi, Type: PERSON\n",
      "Entity: Chewbacca, Type: GPE\n",
      "Entity: Han, Type: NORP\n",
      "Entity: Falcon, Type: ORG\n",
      "Entity: Millennium Falcon, Type: FAC\n",
      "Entity: 1.5, Type: CARDINAL\n",
      "\n",
      "Extracted relations from Star Wars text:\n",
      "Luke (PERSON) --is--> Jedi (PERSON)\n",
      "Master Yoda (PERSON) --is--> Jedi (PERSON)\n",
      "Falcon (ORG) --related_to--> Millennium Falcon (FAC)\n",
      "\n",
      "Knowledge graph triples:\n",
      "Movie --rdf-schema#label--> Movie\n",
      "Chewbacca --rdf-schema#label--> Chewbacca\n",
      "Master Yoda --is--> Jedi\n",
      "Jedi --rdf-schema#label--> Jedi\n",
      "Movie --22-rdf-syntax-ns#type--> http://example.org/Entity\n",
      "Master Yoda --22-rdf-syntax-ns#type--> http://example.org/Person\n",
      "1.5 --rdf-schema#label--> 1.5\n",
      "Jedis --22-rdf-syntax-ns#type--> http://example.org/Person\n",
      "Han --22-rdf-syntax-ns#type--> http://example.org/Entity\n",
      "Luke --22-rdf-syntax-ns#type--> http://example.org/Person\n",
      "Han --rdf-schema#label--> Han\n",
      "Jedi --22-rdf-syntax-ns#type--> http://example.org/Person\n",
      "Jedis --rdf-schema#label--> Jedis\n",
      "Luke --is--> Jedi\n",
      "Falcon --22-rdf-syntax-ns#type--> http://example.org/Organization\n",
      "1.5 --22-rdf-syntax-ns#type--> http://example.org/Entity\n",
      "Chewbacca --22-rdf-syntax-ns#type--> http://example.org/Location\n",
      "Millennium Falcon --rdf-schema#label--> Millennium Falcon\n",
      "Master Yoda --rdf-schema#label--> Master Yoda\n",
      "Luke --rdf-schema#label--> Luke\n",
      "Falcon --related_to--> Millennium Falcon\n",
      "Millennium Falcon --22-rdf-syntax-ns#type--> http://example.org/Entity\n",
      "Falcon --rdf-schema#label--> Falcon\n"
     ]
    }
   ],
   "source": [
    "star_wars_text = \"\"\"Star Wars IV is a Movie where there are different kinds of creatures, like\n",
    "humans and wookies. Some creatures are Jedis; for instance, the human Luke\n",
    "is a Jedi, and Master Yoda – for whom the species is not known – is also a\n",
    "Jedi. The wookie named Chewbacca is Han's co-pilot on the Millennium\n",
    "Falcon starship. The speed of Millennium Falcon is 1.5 (above the speed of\n",
    "light!)\"\"\"\n",
    "\n",
    "print(\"\\nProcessing Star Wars example text...\")\n",
    "kg, entities, relations = build_knowledge_graph(star_wars_text)\n",
    "\n",
    "print(\"\\nExtracted entities from Star Wars text:\")\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity[0]}, Type: {entity[1]}\")\n",
    "\n",
    "print(\"\\nExtracted relations from Star Wars text:\")\n",
    "if relations:\n",
    "    for relation in relations:\n",
    "        print(f\"{relation['subject']} ({relation['subject_type']}) --{relation['predicate']}--> {relation['object']} ({relation['object_type']})\")\n",
    "else:\n",
    "    print(\"No relations found\")\n",
    "\n",
    "print(\"\\nKnowledge graph triples:\")\n",
    "for s, p, o in kg:\n",
    "    s_label = kg.value(s, RDFS.label) or s\n",
    "    p_label = p.split('/')[-1]\n",
    "    o_label = kg.value(o, RDFS.label) or o\n",
    "    print(f\"{s_label} --{p_label}--> {o_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing SPARQL queries on the knowledge graph:\n",
      "\n",
      "Query 1: All Person entities\n",
      "Person: Jedis\n",
      "Person: Luke\n",
      "Person: Jedi\n",
      "Person: Master Yoda\n",
      "\n",
      "Query 2: Relationships between entities\n",
      "Luke --http://example.org/is--> Jedi\n",
      "Master Yoda --http://example.org/is--> Jedi\n",
      "Falcon --http://example.org/related_to--> Millennium Falcon\n",
      "\n",
      "Query 3: Entity type counts\n",
      "Entity: <built-in method count of ResultRow object at 0x321780db0>\n",
      "Person: <built-in method count of ResultRow object at 0x321780ea0>\n",
      "Location: <built-in method count of ResultRow object at 0x321780db0>\n",
      "Organization: <built-in method count of ResultRow object at 0x321780ea0>\n"
     ]
    }
   ],
   "source": [
    "# Perform SPARQL queries\n",
    "print(\"\\nPerforming SPARQL queries on the knowledge graph:\")\n",
    "\n",
    "# Query 1: Find all entities of a specific type\n",
    "query1 = \"\"\"\n",
    "SELECT ?entity ?label\n",
    "WHERE {\n",
    "    ?entity a <http://example.org/Person> .\n",
    "    ?entity <http://www.w3.org/2000/01/rdf-schema#label> ?label .\n",
    "}\n",
    "\"\"\"\n",
    "print(\"\\nQuery 1: All Person entities\")\n",
    "results1 = list(kg.query(query1))\n",
    "if results1:\n",
    "    for row in results1:\n",
    "        print(f\"Person: {row.label}\")\n",
    "else:\n",
    "    print(\"No Person entities found\")\n",
    "\n",
    "# Query 2: Find relationships between entities\n",
    "query2 = \"\"\"\n",
    "SELECT ?s_label ?p ?o_label\n",
    "WHERE {\n",
    "    ?s ?p ?o .\n",
    "    ?s <http://www.w3.org/2000/01/rdf-schema#label> ?s_label .\n",
    "    ?o <http://www.w3.org/2000/01/rdf-schema#label> ?o_label .\n",
    "    FILTER(?p != <http://www.w3.org/2000/01/rdf-schema#label>)\n",
    "    FILTER(?p != <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)\n",
    "}\n",
    "\"\"\"\n",
    "print(\"\\nQuery 2: Relationships between entities\")\n",
    "results2 = list(kg.query(query2))\n",
    "if results2:\n",
    "    for row in results2:\n",
    "        print(f\"{row.s_label} --{row.p}--> {row.o_label}\")\n",
    "else:\n",
    "    print(\"No relationships found\")\n",
    "\n",
    "# Query 3: Find all entity types and their counts\n",
    "query3 = \"\"\"\n",
    "SELECT ?type (COUNT(?entity) as ?count)\n",
    "WHERE {\n",
    "    ?entity a ?type .\n",
    "}\n",
    "GROUP BY ?type\n",
    "\"\"\"\n",
    "print(\"\\nQuery 3: Entity type counts\")\n",
    "for row in kg.query(query3):\n",
    "    type_name = row.type.split('/')[-1]\n",
    "    print(f\"{type_name}: {row.count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is performing well enough!\n",
    "\n",
    "- **Query 1:** successfully found Person entities: Jedis, Luke, Jedi, and Master Yoda\n",
    "- **Query 2:** found meaningfull relations\n",
    "  - Luke is Jedi\n",
    "  - Master Yoda is Jedi\n",
    "  - Falcon is related to Millenium Falcon\n",
    "- **Query 3:** shows that we have a good distribution of entity types: Entity, Person, Location and Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping to Collect real-world data for our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to scrape from The Guardian...\n",
      "Scraping The Guardian technology section...\n",
      "Found 0 potential article links\n",
      "Not enough articles from The Guardian. Trying CNN...\n",
      "Scraping CNN tech section...\n",
      "Found 48 potential article links\n",
      "Processing article: Trump announces new auto tariffs in a major trade war escalation\n",
      "Added article #1: Trump announces new auto tariffs in a major trade war escalation\n",
      "Processing article: It’s the world’s hottest car company. You can’t buy one in America\n",
      "Added article #2: It’s the world’s hottest car company. You can’t buy one in America\n",
      "Processing article: Copper prices surge to record high amid tariff anxiety\n",
      "Added article #3: Copper prices surge to record high amid tariff anxiety\n",
      "Processing article: Gabby Jones/Bloomberg/Getty Images\n",
      "Added article #4: Gabby Jones/Bloomberg/Getty Images\n",
      "Processing article: Sheldon Cooper/SOPA Images/LightRocket/Getty Images/File\n",
      "Added article #5: Sheldon Cooper/SOPA Images/LightRocket/Getty Images/File\n",
      "Processing article: Tiffany Hagler-Geard/Bloomberg/Getty Images\n",
      "Added article #6: Tiffany Hagler-Geard/Bloomberg/Getty Images\n",
      "Processing article: Bridget Bennett/Bloomberg/Getty Images\n",
      "Added article #7: Bridget Bennett/Bloomberg/Getty Images\n",
      "Processing article: Hou Yu/China News Service/VCG/Getty Images\n",
      "Added article #8: Hou Yu/China News Service/VCG/Getty Images\n",
      "Processing article: George Frey/Reuters\n",
      "Added article #9: George Frey/Reuters\n",
      "Processing article: Justin Sullivan/Getty Images\n",
      "Added article #10: Justin Sullivan/Getty Images\n",
      "\n",
      "Successfully scraped 10 articles\n",
      "\n",
      "Article 1: Trump announces new auto tariffs in a major trade war escalation\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/03/26/economy/auto-tariffs-announcement/index.html\n",
      "Content Preview: President Donald Trump on Wednesday announced 25% tariffs on all cars shipped to the United States, a significant escalation in a global trade war. Th...\n",
      "\n",
      "Article 2: It’s the world’s hottest car company. You can’t buy one in America\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/03/26/cars/china-byd-profile-tesla-rival-intl-hnk/index.html\n",
      "Content Preview: In the world of electric vehicles, there’s a Chinese company outdoing Elon Musk’s Tesla. And it’s just getting started. BYD, the Shenzhen-based Chines...\n",
      "\n",
      "Article 3: Copper prices surge to record high amid tariff anxiety\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/03/26/investing/copper-record-high-tariff-anxiety/index.html\n",
      "Content Preview: Gold isn’t the only metal smashing through record high prices. Copper prices in New York hit a record high on Wednesday after Bloomberg reported that ...\n",
      "\n",
      "Article 4: Gabby Jones/Bloomberg/Getty Images\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/03/25/tech/how-to-use-signal-app-privacy-encryption/index.html\n",
      "Content Preview: If you hadn’t heard of Signal before Monday, chances are you have now. The secure messaging app, available for iPhones and Android devices, has long b...\n",
      "\n",
      "Article 5: Sheldon Cooper/SOPA Images/LightRocket/Getty Images/File\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/02/09/tech/secure-chat-apps-signal-tor-browser/index.html\n",
      "Content Preview: As we all live more of our lives online, it’s important to understand who might have access to our conversations and internet searches – and to unders...\n",
      "\n",
      "Article 6: Tiffany Hagler-Geard/Bloomberg/Getty Images\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/03/25/tech/23andme-bankruptcy-how-to-delete-data/index.html\n",
      "Content Preview: Many 23andMe customers signed up to the genetic testing service in hopes of learning fun or interesting information about their past. But consumer adv...\n",
      "\n",
      "Article 7: Bridget Bennett/Bloomberg/Getty Images\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/03/24/tech/samsung-co-ceo-han-jong-hee-death-intl-hnk/index.html\n",
      "Content Preview: Samsung Electronics co-CEO Han Jong-Hee died from cardiac arrest on Tuesday, according to a spokesperson for the South Korean tech giant. Han was 63. ...\n",
      "\n",
      "Article 8: Hou Yu/China News Service/VCG/Getty Images\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/03/25/tech/china-robots-market-competitiveness-intl-hnk/index.html\n",
      "Content Preview: Spinning bright red handkerchiefs and dancing in step to folk music, more than a dozen human-like robots took to China’s biggest stage in January, mak...\n",
      "\n",
      "Article 9: George Frey/Reuters\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/03/24/tech/23andme-bankruptcy-hnk-intl/index.html\n",
      "Content Preview: Gene testing firm 23andMe said on Sunday it had filed for Chapter 11 bankruptcy protection in order to facilitate its sale, after years of struggling ...\n",
      "\n",
      "Article 10: Justin Sullivan/Getty Images\n",
      "Date: Unknown\n",
      "URL: https://www.cnn.com/2025/03/20/tech/meta-whistleblower-sarah-wynn-williams-response-congress/index.html\n",
      "Content Preview: Former Facebook executive-turned-whistleblower Sarah Wynn-Williams says Meta is blocking her from speaking to Congress about her experiences at the co...\n",
      "\n",
      "Articles saved to 'scraped_articles.json'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "def scrape_guardian_news(category=\"technology\", num_articles=10):\n",
    "    \"\"\"\n",
    "    Scrape news articles from The Guardian.\n",
    "    \n",
    "    Args:\n",
    "        category: News category (e.g., 'technology', 'business', 'science')\n",
    "        num_articles: Maximum number of articles to scrape\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing article data\n",
    "    \"\"\"\n",
    "    url = f\"https://www.theguardian.com/{category}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Scraping The Guardian {category} section...\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = []\n",
    "        \n",
    "        # The Guardian's article cards\n",
    "        article_elements = soup.select(\"div.fc-item__container\")\n",
    "        \n",
    "        if not article_elements:\n",
    "            # Try alternative selectors\n",
    "            article_elements = soup.select(\".js-headline-text\") or soup.select(\"a.u-faux-block-link__overlay\")\n",
    "        \n",
    "        print(f\"Found {len(article_elements)} potential article links\")\n",
    "        \n",
    "        # Process each article\n",
    "        count = 0\n",
    "        processed_urls = set()  # To avoid duplicates\n",
    "        \n",
    "        for article_elem in article_elements:\n",
    "            if count >= num_articles:\n",
    "                break\n",
    "            \n",
    "            # Find the link - either the element itself is a link or it contains a link\n",
    "            if article_elem.name == 'a':\n",
    "                link_elem = article_elem\n",
    "            else:\n",
    "                link_elem = article_elem.find('a')\n",
    "            \n",
    "            if not link_elem:\n",
    "                continue\n",
    "                \n",
    "            article_url = link_elem.get('href')\n",
    "            \n",
    "            # Skip if we don't have a URL or already processed this URL\n",
    "            if not article_url or article_url in processed_urls:\n",
    "                continue\n",
    "                \n",
    "            processed_urls.add(article_url)\n",
    "            \n",
    "            # Make sure URL is complete\n",
    "            if not article_url.startswith('http'):\n",
    "                article_url = f\"https://www.theguardian.com{article_url}\"\n",
    "            \n",
    "            # Extract title from headline element or link text\n",
    "            headline = link_elem.select_one('.js-headline-text') or link_elem.select_one('h3')\n",
    "            title = headline.text.strip() if headline else link_elem.text.strip()\n",
    "            \n",
    "            # Skip if title is too short or empty\n",
    "            if not title or len(title) < 10:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing article: {title}\")\n",
    "            \n",
    "            try:\n",
    "                # Get the full article page\n",
    "                article_response = requests.get(article_url, headers=headers)\n",
    "                article_response.raise_for_status()\n",
    "                \n",
    "                article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "                \n",
    "                # Extract article content - Guardian usually uses div.content__article-body\n",
    "                content_blocks = article_soup.select(\"div.content__article-body p, div.article-body-commercial-selector p\")\n",
    "                \n",
    "                if not content_blocks:\n",
    "                    # Try alternative selectors\n",
    "                    content_blocks = (\n",
    "                        article_soup.select(\".article-body p\") or \n",
    "                        article_soup.select(\"article.content--article p\")\n",
    "                    )\n",
    "                \n",
    "                content = \" \".join([block.text.strip() for block in content_blocks])\n",
    "                \n",
    "                # Get publication date\n",
    "                time_elem = article_soup.select_one(\"time\")\n",
    "                publication_date = time_elem.get('datetime') if time_elem else \"Unknown\"\n",
    "                \n",
    "                # Add article if we have meaningful content\n",
    "                if content and len(content) > 150:\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': article_url,\n",
    "                        'content': content,\n",
    "                        'publication_date': publication_date\n",
    "                    })\n",
    "                    count += 1\n",
    "                    print(f\"Added article #{count}: {title}\")\n",
    "                    \n",
    "                # Be respectful with rate limiting\n",
    "                time.sleep(random.uniform(1.0, 2.0))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping article {article_url}: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping The Guardian: {e}\")\n",
    "        return []\n",
    "\n",
    "# Alternative: Try CNN if The Guardian doesn't work\n",
    "def scrape_cnn_news(category=\"tech\", num_articles=10):\n",
    "    \"\"\"Scrape news articles from CNN.\"\"\"\n",
    "    url = f\"https://www.cnn.com/{category}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Scraping CNN {category} section...\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = []\n",
    "        \n",
    "        # CNN's article links\n",
    "        article_links = soup.select(\"a.container__link, .container__headline a\")\n",
    "        \n",
    "        print(f\"Found {len(article_links)} potential article links\")\n",
    "        \n",
    "        count = 0\n",
    "        processed_urls = set()\n",
    "        \n",
    "        for link in article_links:\n",
    "            if count >= num_articles:\n",
    "                break\n",
    "                \n",
    "            article_url = link.get('href')\n",
    "            \n",
    "            if not article_url or article_url in processed_urls:\n",
    "                continue\n",
    "                \n",
    "            processed_urls.add(article_url)\n",
    "            \n",
    "            if not article_url.startswith('http'):\n",
    "                article_url = f\"https://www.cnn.com{article_url}\"\n",
    "            \n",
    "            # Get the headline text\n",
    "            headline = link.select_one('.container__headline-text')\n",
    "            title = headline.text.strip() if headline else link.text.strip()\n",
    "            \n",
    "            if not title or len(title) < 10:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing article: {title}\")\n",
    "            \n",
    "            try:\n",
    "                article_response = requests.get(article_url, headers=headers)\n",
    "                article_response.raise_for_status()\n",
    "                \n",
    "                article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "                \n",
    "                # CNN article content\n",
    "                content_blocks = article_soup.select(\".article__content p, .zn-body__paragraph\")\n",
    "                \n",
    "                content = \" \".join([block.text.strip() for block in content_blocks])\n",
    "                \n",
    "                # Get publication date\n",
    "                time_elem = article_soup.select_one(\"time\") or article_soup.select_one(\".update-time\")\n",
    "                publication_date = time_elem.get('datetime') if time_elem and time_elem.has_attr('datetime') else time_elem.text.strip() if time_elem else \"Unknown\"\n",
    "                \n",
    "                if content and len(content) > 150:\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': article_url,\n",
    "                        'content': content,\n",
    "                        'publication_date': publication_date\n",
    "                    })\n",
    "                    count += 1\n",
    "                    print(f\"Added article #{count}: {title}\")\n",
    "                    \n",
    "                time.sleep(random.uniform(1.0, 2.0))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping article {article_url}: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping CNN: {e}\")\n",
    "        return []\n",
    "\n",
    "# Let's try multiple news sources until we get enough articles\n",
    "def get_news_articles(num_articles=10):\n",
    "    \"\"\"Try multiple news sources to get articles.\"\"\"\n",
    "    print(\"Trying to scrape from The Guardian...\")\n",
    "    articles = scrape_guardian_news(category=\"technology\", num_articles=num_articles)\n",
    "    \n",
    "    if not articles or len(articles) < 5:\n",
    "        print(\"Not enough articles from The Guardian. Trying CNN...\")\n",
    "        cnn_articles = scrape_cnn_news(category=\"tech\", num_articles=num_articles)\n",
    "        articles.extend(cnn_articles)\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Get the articles\n",
    "articles = get_news_articles(num_articles=10)\n",
    "\n",
    "# Print results\n",
    "if articles:\n",
    "    print(f\"\\nSuccessfully scraped {len(articles)} articles\")\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        print(f\"\\nArticle {i+1}: {article['title']}\")\n",
    "        print(f\"Date: {article['publication_date']}\")\n",
    "        print(f\"URL: {article['url']}\")\n",
    "        print(f\"Content Preview: {article['content'][:150]}...\")\n",
    "    \n",
    "    # Save articles to a file for backup\n",
    "    with open(\"scraped_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\nArticles saved to 'scraped_articles.json'\")\n",
    "else:\n",
    "    print(\"Failed to scrape articles. Please check your internet connection or try with a different approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Knowledge Graph Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building knowledge graph from scraped articles...\n",
      "\n",
      "Processing article 1/10: Trump announces new auto tariffs in a major trade war escalation\n",
      "\n",
      "Processing article 2/10: It’s the world’s hottest car company. You can’t buy one in America\n",
      "\n",
      "Processing article 3/10: Copper prices surge to record high amid tariff anxiety\n",
      "\n",
      "Processing article 4/10: Gabby Jones/Bloomberg/Getty Images\n",
      "\n",
      "Processing article 5/10: Sheldon Cooper/SOPA Images/LightRocket/Getty Images/File\n",
      "\n",
      "Processing article 6/10: Tiffany Hagler-Geard/Bloomberg/Getty Images\n",
      "\n",
      "Processing article 7/10: Bridget Bennett/Bloomberg/Getty Images\n",
      "\n",
      "Processing article 8/10: Hou Yu/China News Service/VCG/Getty Images\n",
      "\n",
      "Processing article 9/10: George Frey/Reuters\n",
      "\n",
      "Processing article 10/10: Justin Sullivan/Getty Images\n",
      "\n",
      "Knowledge Graph Statistics:\n",
      "Total entities: 905\n",
      "Entity types:\n",
      "  PERSON: 111\n",
      "  DATE: 140\n",
      "  PERCENT: 28\n",
      "  GPE: 155\n",
      "  TIME: 6\n",
      "  CARDINAL: 61\n",
      "  ORG: 285\n",
      "  NORP: 60\n",
      "  LOC: 7\n",
      "  MONEY: 27\n",
      "  PRODUCT: 3\n",
      "  QUANTITY: 5\n",
      "  WORK_OF_ART: 7\n",
      "  ORDINAL: 6\n",
      "  LAW: 2\n",
      "  EVENT: 1\n",
      "  FAC: 1\n",
      "Total relations: 215\n",
      "Total triples in graph: 1592\n",
      "\n",
      "Knowledge graph saved to 'news_knowledge_graph.ttl'\n",
      "\n",
      "Performing SPARQL queries on the knowledge graph...\n",
      "\n",
      "Query 1: Top 10 most frequently mentioned entities\n",
      "one: 7 mentions\n",
      "One: 7 mentions\n",
      "CNN: 7 mentions\n",
      "US: 5 mentions\n",
      "U.S.: 5 mentions\n",
      "China: 5 mentions\n",
      "years: 5 mentions\n",
      "Chinese: 4 mentions\n",
      "2025: 4 mentions\n",
      "last month: 4 mentions\n",
      "\n",
      "Query 2: Entity counts by type\n",
      "Entity: <built-in method count of ResultRow object at 0x322ec8310>\n",
      "Organization: <built-in method count of ResultRow object at 0x322ec8220>\n",
      "Person: <built-in method count of ResultRow object at 0x322ec8310>\n",
      "Location: <built-in method count of ResultRow object at 0x322ec8220>\n",
      "Article: <built-in method count of ResultRow object at 0x322ec8310>\n",
      "\n",
      "Query 3: Sample relationships between entities\n",
      "Honda --lost--> 2.5%\n",
      "HMC --related_to--> 2.5%\n",
      "Honda --lost--> 25%\n",
      "HMC --related_to--> 25%\n",
      "Mexico --related_to--> 2024\n",
      "Americans --related_to--> 2024\n",
      "$107 billion --reported_for--> 2024\n",
      "$107 billion --related_to--> 2024\n",
      "32% --related_to--> last year\n",
      "last year --exported_to--> Mexico\n",
      "$35.8 billion --related_to--> Mexico\n",
      "$28.4 billion --exported_to--> Mexico\n",
      "US --related_to--> Mexico\n",
      "US --exported_to--> Mexico\n",
      "Chevrolet --related_to--> Mexico\n",
      "Beijing --delaying_in--> Mexico\n",
      "4 million --related_to--> Mexico\n",
      "U.S. --related_to--> Mexico\n",
      "U.S. --exported_to--> Mexico\n",
      "Jeff Bezos --related_to--> American\n",
      "\n",
      "Query 4: Top entity co-occurrences\n",
      "China & US: 5 articles\n",
      "China & U.S.: 5 articles\n",
      "US & one: 4 articles\n",
      "U.S. & one: 4 articles\n",
      "One & US: 4 articles\n",
      "One & U.S.: 4 articles\n",
      "Chinese & US: 4 articles\n",
      "Chinese & U.S.: 4 articles\n",
      "China & one: 4 articles\n",
      "China & One: 4 articles\n",
      "China & Chinese: 4 articles\n",
      "CNN & US: 4 articles\n",
      "CNN & U.S.: 4 articles\n",
      "CNN & one: 4 articles\n",
      "CNN & One: 4 articles\n",
      "\n",
      "Query 5: Top 10 most mentioned locations\n",
      "China: 5 mentions\n",
      "US: 5 mentions\n",
      "U.S.: 5 mentions\n",
      "the United States: 3 mentions\n",
      "Beijing: 3 mentions\n",
      "Mexico: 2 mentions\n",
      "California: 2 mentions\n",
      "Shenzhen: 2 mentions\n",
      "New York: 2 mentions\n",
      "Bloomberg: 2 mentions\n"
     ]
    }
   ],
   "source": [
    "def process_articles_to_knowledge_graph(articles):\n",
    "    \"\"\"\n",
    "    Process the scraped articles through our NLP pipeline and build a knowledge graph.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of dictionaries containing article data\n",
    "        \n",
    "    Returns:\n",
    "        The complete knowledge graph\n",
    "    \"\"\"\n",
    "    print(\"\\nBuilding knowledge graph from scraped articles...\")\n",
    "    \n",
    "    # Create a new RDF graph for our combined knowledge graph\n",
    "    from rdflib import Graph, URIRef, Literal, Namespace\n",
    "    from rdflib.namespace import RDF, RDFS, XSD\n",
    "    \n",
    "    combined_graph = Graph()\n",
    "    EX = Namespace(\"http://example.org/\")\n",
    "    combined_graph.bind(\"ex\", EX)\n",
    "    \n",
    "    # Track all entities and relations for statistics\n",
    "    all_entities = []\n",
    "    all_relations = []\n",
    "    \n",
    "    # Process each article\n",
    "    for i, article in enumerate(articles):\n",
    "        print(f\"\\nProcessing article {i+1}/{len(articles)}: {article['title']}\")\n",
    "        \n",
    "        # Get the content\n",
    "        content = article['content']\n",
    "        \n",
    "        # Build a knowledge graph for this article\n",
    "        article_graph, entities, relations = build_knowledge_graph(content)\n",
    "        \n",
    "        # Track entities and relations\n",
    "        all_entities.extend(entities)\n",
    "        all_relations.extend(relations)\n",
    "        \n",
    "        # Create an article entity in the knowledge graph\n",
    "        article_uri = URIRef(EX[f\"article_{i+1}\"])\n",
    "        combined_graph.add((article_uri, RDF.type, EX.Article))\n",
    "        combined_graph.add((article_uri, RDFS.label, Literal(article['title'])))\n",
    "        combined_graph.add((article_uri, EX.url, Literal(article['url'])))\n",
    "        combined_graph.add((article_uri, EX.publishedDate, Literal(article['publication_date'])))\n",
    "        \n",
    "        # Add all triples from the article's graph to the combined graph\n",
    "        for s, p, o in article_graph:\n",
    "            combined_graph.add((s, p, o))\n",
    "            \n",
    "            # Link entities to the article they were mentioned in\n",
    "            if isinstance(s, URIRef) and not str(s).endswith(f\"article_{i+1}\"):\n",
    "                combined_graph.add((s, EX.mentionedIn, article_uri))\n",
    "    \n",
    "    # Print statistics\n",
    "    entity_types = {}\n",
    "    for _, entity_type, _, _ in all_entities:\n",
    "        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nKnowledge Graph Statistics:\")\n",
    "    print(f\"Total entities: {len(all_entities)}\")\n",
    "    print(\"Entity types:\")\n",
    "    for entity_type, count in entity_types.items():\n",
    "        print(f\"  {entity_type}: {count}\")\n",
    "    \n",
    "    print(f\"Total relations: {len(all_relations)}\")\n",
    "    print(f\"Total triples in graph: {len(combined_graph)}\")\n",
    "    \n",
    "    # Save the knowledge graph to a file\n",
    "    combined_graph.serialize(destination=\"news_knowledge_graph.ttl\", format=\"turtle\")\n",
    "    print(\"\\nKnowledge graph saved to 'news_knowledge_graph.ttl'\")\n",
    "    \n",
    "    return combined_graph, all_entities, all_relations\n",
    "\n",
    "# Process the articles and build our knowledge graph\n",
    "if articles:\n",
    "    knowledge_graph, all_entities, all_relations = process_articles_to_knowledge_graph(articles)\n",
    "    \n",
    "    # Run SPARQL queries to analyze the knowledge graph\n",
    "    print(\"\\nPerforming SPARQL queries on the knowledge graph...\")\n",
    "    \n",
    "    # Query 1: Find the most frequently mentioned entities\n",
    "    query1 = \"\"\"\n",
    "    SELECT ?entity ?label (COUNT(?article) as ?mentions)\n",
    "    WHERE {\n",
    "        ?entity <http://example.org/mentionedIn> ?article .\n",
    "        ?entity <http://www.w3.org/2000/01/rdf-schema#label> ?label .\n",
    "    }\n",
    "    GROUP BY ?entity ?label\n",
    "    ORDER BY DESC(?mentions)\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 1: Top 10 most frequently mentioned entities\")\n",
    "    results1 = list(knowledge_graph.query(query1))\n",
    "    if results1:\n",
    "        for row in results1:\n",
    "            print(f\"{row.label}: {row.mentions} mentions\")\n",
    "    else:\n",
    "        print(\"No entity mentions found\")\n",
    "    \n",
    "    # Query 2: Find entities by type\n",
    "    query2 = \"\"\"\n",
    "    SELECT ?type (COUNT(?entity) as ?count)\n",
    "    WHERE {\n",
    "        ?entity a ?type .\n",
    "    }\n",
    "    GROUP BY ?type\n",
    "    ORDER BY DESC(?count)\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 2: Entity counts by type\")\n",
    "    for row in knowledge_graph.query(query2):\n",
    "        type_name = str(row.type).split('/')[-1]\n",
    "        print(f\"{type_name}: {row.count}\")\n",
    "    \n",
    "    # Query 3: Find relationships between entities\n",
    "    query3 = \"\"\"\n",
    "    SELECT ?s_label ?p ?o_label\n",
    "    WHERE {\n",
    "        ?s ?p ?o .\n",
    "        ?s <http://www.w3.org/2000/01/rdf-schema#label> ?s_label .\n",
    "        ?o <http://www.w3.org/2000/01/rdf-schema#label> ?o_label .\n",
    "        FILTER(?p != <http://www.w3.org/2000/01/rdf-schema#label>)\n",
    "        FILTER(?p != <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)\n",
    "        FILTER(?p != <http://example.org/mentionedIn>)\n",
    "    }\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 3: Sample relationships between entities\")\n",
    "    results3 = list(knowledge_graph.query(query3))\n",
    "    if results3:\n",
    "        for row in results3:\n",
    "            p_name = str(row.p).split('/')[-1]\n",
    "            print(f\"{row.s_label} --{p_name}--> {row.o_label}\")\n",
    "    else:\n",
    "        print(\"No relationships found\")\n",
    "    \n",
    "    # Query 4: Find co-occurring entities (entities mentioned in the same article)\n",
    "    query4 = \"\"\"\n",
    "    SELECT ?entity1_label ?entity2_label (COUNT(?article) as ?co_occurrences)\n",
    "    WHERE {\n",
    "        ?entity1 <http://example.org/mentionedIn> ?article .\n",
    "        ?entity2 <http://example.org/mentionedIn> ?article .\n",
    "        ?entity1 <http://www.w3.org/2000/01/rdf-schema#label> ?entity1_label .\n",
    "        ?entity2 <http://www.w3.org/2000/01/rdf-schema#label> ?entity2_label .\n",
    "        FILTER(?entity1 != ?entity2)\n",
    "        FILTER(?entity1_label < ?entity2_label)  # To avoid duplicates\n",
    "    }\n",
    "    GROUP BY ?entity1_label ?entity2_label\n",
    "    ORDER BY DESC(?co_occurrences)\n",
    "    LIMIT 15\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 4: Top entity co-occurrences\")\n",
    "    results4 = list(knowledge_graph.query(query4))\n",
    "    if results4:\n",
    "        for row in results4:\n",
    "            print(f\"{row.entity1_label} & {row.entity2_label}: {row.co_occurrences} articles\")\n",
    "    else:\n",
    "        print(\"No co-occurring entities found\")\n",
    "    \n",
    "    # Query 5: Find the most mentioned locations\n",
    "    query5 = \"\"\"\n",
    "    SELECT ?location ?label (COUNT(?article) as ?mentions)\n",
    "    WHERE {\n",
    "        ?location a <http://example.org/Location> .\n",
    "        ?location <http://example.org/mentionedIn> ?article .\n",
    "        ?location <http://www.w3.org/2000/01/rdf-schema#label> ?label .\n",
    "    }\n",
    "    GROUP BY ?location ?label\n",
    "    ORDER BY DESC(?mentions)\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    print(\"\\nQuery 5: Top 10 most mentioned locations\")\n",
    "    results5 = list(knowledge_graph.query(query5))\n",
    "    if results5:\n",
    "        for row in results5:\n",
    "            print(f\"{row.label}: {row.mentions} mentions\")\n",
    "    else:\n",
    "        print(\"No location mentions found\")\n",
    "else:\n",
    "    print(\"Cannot build knowledge graph without articles. Please run the web scraping step again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this does is:\n",
    "1. Processes all scraped articles though out entity and relation extraction pipeline\n",
    "2. Combines them into a unified knowledge graph\n",
    "3. Adds metadata about the articles, inluding links from entitites to the articles they appear in\n",
    "4. Runs comprehensive SPARQL queries to analyze the graph\n",
    "5. Calculates graph statistics like density and entity type distribution\n",
    "6. Provides a summary of the knowledge graph's contents and potential applications\n",
    "\n",
    "The queries are designed to extract useful insights:\n",
    "- most frequently mentioned entities\n",
    "- entitiy counts by type\n",
    "- sample relationships between entities\n",
    "- co-occuring entities (which could suggest hidden relationships)\n",
    "- top mentioned locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "We have:\n",
    "1. Setup our environment: We setup the necessary libraries and dependencies\n",
    "2. Cleaned / preprocessed the text: We implemented text cleaning functions\n",
    "3. NER: We compared spaCy's pretrained NER model with a CRF model\n",
    "4. Relation extraction: We implemented relation extraction using spaCy's dependency parsing\n",
    "5. Knowledge graph building: We constructed an RDF graph and ran SPARQL queries on it\n",
    "6. Webscraping: We implemented functions to scrape news articles from 2 websites (still doesn't work)\n",
    "7. Complete pipeline: We combined all the above components into a unified workflow\n",
    "\n",
    "For our project report we should include:\n",
    "1. A description of our methodology for each step\n",
    "2. The challenges we faced (like the CRF model implementation and relation extraction patterns)\n",
    "3. Sample queries and results from our knowledge graph\n",
    "4. Examples of entities and relations extacted from our scraped articles\n",
    "5. Potential applications and future improvements for our knowledge ggraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Challenge with Relation Extraction\n",
    "Initially, our relation extraction approach was quite limited. We were primarily looking for simple subject-verb-object patterns in the dependency parse tree, but this wasn't capturing many of the actual relationships in the text. When we tested it on our sample sentences, we got \"No relations found\" for all of them.\n",
    "\n",
    "\n",
    "#### The Improvements We Made\n",
    "We enhanced our relation extraction function in several key ways:\n",
    "1. **Entity-Focused Approach** : Rather than trying to parse general sentence structure, we shifted to specifically looking for relationships between named entities that spaCy had already identified. This focused our extraction on meaningful relations between important entities rather than trying to extract all possible sentence relationships.\n",
    "2. **Multiple Pattern Recognition** : We added several additional patterns beyond the basic subject-verb-object:\n",
    "   - Direct subject-verb-object patterns: Captures basic statements like \"Apple was founded by Steve Jobs\"\n",
    "   - Entity-verb-prep-Entity patterns: Captures statements like \"Amazon has headquarters in Seattle\"\n",
    "   - Adjacent entity relationships: Identifies relationships between entities that appear close to each other, such as \"Elon Musk, CEO of Tesla\"\n",
    "3. **Tracking Entity Provenance** : We maintained a mapping between entity tokens and their full entity mentions, which helped us properly reconstruct the relationships between the full entity names rather than just the individual tokens.\n",
    "4. **Sentence-Level Processing** : We processed text sentence by sentence, which helped maintain the correct contextual relationships and prevented incorrect connections across sentence boundaries.\n",
    "5. **Flexible Relationship Types** : When we couldn't determine a specific relationship type from the text, we used a generic \"related_to\" predicate. This allowed us to still capture the relationship between entities even when the exact nature was ambiguous.\n",
    "\n",
    "#### The Results:\n",
    "After implementing these improvements, our relation extraction started successfully identifying meaningful relationships:\n",
    "\n",
    "- We captured \"Elon Musk (PERSON) --related_to--> Texas (GPE)\" from \"Tesla and SpaceX are companies founded by Elon Musk who lives in Texas.\"\n",
    "- We identified \"Amazon (ORG) --related_to--> Seattle (GPE)\" from \"Amazon has its headquarters in Seattle, Washington.\"\n",
    "\n",
    "These improvements turned the relation extraction from a non-functioning component to a key part of our knowledge graph pipeline. It allowed us to populate our graph with meaningful connections between entities, rather than just having isolated entity nodes.\n",
    "\n",
    "#### Limitations and Future Improvements\n",
    "While our approach improved greatly, it still has limitations:\n",
    "1. **Relationship Specificity** : Many relationships are tagged with the generic \"related_to\" predicate rather than more specific relationships.\n",
    "2. **Complex Relationship Structures** : Our current patterns don't handle more complex linguistic structures like coreference (when a pronoun refers to a previously mentioned entity) or nested relationships.\n",
    "3. **Context Sensitivity** : The current extraction doesn't consider broader document context that might clarify relationships.\n",
    "\n",
    "To further improve the relation extraction, we coould:\n",
    "1. Train a supervised relation extraction model on labeled data\n",
    "2. Implement more sophisticated linguistic patterns\n",
    "3. Add coreference resolution to connect pronouns to their referenced entities\n",
    "4. Use pre-trained models specifically designed for relation extraction\n",
    "\n",
    "Despite these limitations, our improved relation extraction approach successfully extracts meaningful relationships that form the backbone of our knowledge graph, which was our primary goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
